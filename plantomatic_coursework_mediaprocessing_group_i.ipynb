{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLANTOMATIC_coursework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/plantomatic_coursework_mediaprocessing_group_i.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp_M7EMFQTCF"
      },
      "source": [
        "**P L A N T O M A T I C** - COURSEWORK\n",
        "\n",
        "---\n",
        "\n",
        "*Kian Lütke, Carsten Montag, Johannes Maximilian Stürenburg*\n",
        "\n",
        "This document outlines the documentation and progress journal of group I of the class _Media-Processing_. This work is divided in several chapters. Each section will be presented with executable code examples which will be - if run in the correct order - in the end illustrate the whole project together. \n",
        "\n",
        "The same code will be appended in the appendix to be run alone; not tied to this course work.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFJa8RtR7Q8b"
      },
      "source": [
        "# 1. Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7velBbyDxDX"
      },
      "source": [
        "## 1.1. Motivation\n",
        "\n",
        "This coursework is part of a multi-project class called _Mediaprocessing_ taught by Prof. Dr. techn. Susanne Boll-Westermann and Dr.-Ing. Larbi Abdenebaoui. During the introduction weeks several Projects were introduced to pick from. As one of the group members does grow herbs at home the obvious choice here is the project _How Is My Plant Doing_. To briefly explain the goal subject: An image of a plant should be analyzed to tell whether the plant needs watering or not.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAK4eJcPD0lL"
      },
      "source": [
        "## 1.2. Case/Goal\n",
        "\n",
        "The main goal of the project is to distinguish between two different plants and classify them to get an idea about their current state. \n",
        "\n",
        "A case was developed to fund the classwork’s structure upon:\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/1-concept.png?raw=1\" height=\"300px\" style=\"margin: 3em;\">\n",
        "\n",
        "**Fig 1** - Rough Concept\n",
        "\n",
        "The idea outlines that some sort of robot will take care of a kitchen-garden. The robot should move from plant to plant taking pictures and processing the images. Afterwards the images will be analyzed and classified. The robot decides whether the plant needs to be watered or not. In the letter case a message will be send to the owner’s mobile device to inform about the plants state. The message could contain the image of the plant itself as well. As there is no access to a real robot which actually is able to move. This project will make use of a stationary Raspberry Pi computer with a camera attached to it. The Raspberry Pi will then take a set of pictures every hour and evaluate the plant-state directly.    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8EpJUczD3Na"
      },
      "source": [
        "## 1.3. Methodology\n",
        "This work is divided in two main sections: The feature analysis part and the machine learning part. The first part will cover how images were gathered and how these images will be processed in order to prepare them for the machine learning training. In the second part the actual learning of the machine learning algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKoiNLFw7zYC"
      },
      "source": [
        "# 2. Feature Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnXBdCFCEADR"
      },
      "source": [
        "\n",
        "## 2.1. Data Gathering\n",
        "\n",
        "This section describes the process of acquiring images of the plants. The first approach was to use a mirrorless system-camera to take thousands of pictures.\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/dlsm_basilikum.jpg?raw=1\" height=\"300px\" style=\"margin: 3em;\">\n",
        "\n",
        "**Fig 2** - Picture taken with a DSLM Full Frame Camera [Sony a7 – 50mm/f5.6]\n",
        "\n",
        "As seen in Figure 2 the results are astonishing. The downside comes with the amount of time needed to take all the needed images. Taking approx. 1200 images took around 3 hours. So, the idea came up to take short videos of the plant and extract single frames to process them as images. For example, a short clip of 30 seconds could generate 1800 images if every fame is used.\n",
        "\n",
        "60 FPS Footage: 30 (seconds) * 60 (fps) = 1800\n",
        "\n",
        "The drawback here is the blurriness of extracted frames. A solution will be discussed in the next section.\n",
        "\n",
        "**Choosing the right plants**\n",
        "\n",
        "At the beginning of the projects two different plants were chosen: Basil and German mint. During this project we decided – because of the difficulties to distinguish between this two plants  - to switch from mint to chives. We decided to create a diary and observe the plants in different states. More on that in the next chapter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oE5qIdzEENf"
      },
      "source": [
        "## 2.2. The Plant-Diary\n",
        "\n",
        "Thie first report consists of two different plants. One chive and one basil. They were captured on video in different time-intervals. The expected outcome should be a good base to start the training of later models.\n",
        "\n",
        "### Diary One\n",
        "\n",
        "> Date | State of Chive | Sate of Basil | Video Chive | Video Basil | Notes | Time passed (h) since recovery \n",
        "> --- | --- | --- | --- | --- | --- | ---\n",
        "> 09.12.20 12:30 | New | Needs watering | ------------------------------------------------------ | ------------------------------------------------------ | Last watering | n/a\n",
        "> 09.12.20 17:00 | No change visible | Fully recovered | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch1.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum1.gif?raw=1\" height=\"100\"> |  | 0\n",
        "> 10.12.20 12:10 | No change visible | No change visible | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch2.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum2.gif?raw=1\" height=\"100\"> |  | 19\n",
        "> 10.12.20 19:50 | Visible hanging of the leaves | Pot is very light, will need water soon | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch3.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum3.gif?raw=1\" height=\"100\"> |  | 27\n",
        "> 11.12.20 11:50 | Even more hanging | Leaves also start hanging; pot is very light; needs water | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch4.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum4.gif?raw=1\" height=\"100\"> |  | 43\n",
        "> 12.12.20 09:10 | Still green; growing fast | End of experiment; final state is reached | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch5.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum5.gif?raw=1\" height=\"100\"> | Basil gets watered | 64\n",
        "> 13.12.20 14:30 | Still hanging | Fully recovered again | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch6.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum6.gif?raw=1\" height=\"100\"> | Chive gets watered | 93\n",
        "> 14.12.20 12:30 | Still the same development | aborted | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch7.gif?raw=1\" height=\"100\">  | n/a |  | 116\n",
        "> 16.12.20 08:30 | Still the same development | aborted | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch8.gif?raw=1\" height=\"100\">  | n/a | Experiment aborted | 159\n",
        "\n",
        "The table shows different development between the two plants. The chive grew a lot and hat brownish stalks. The basil on the other hand indicated the lack of water with hanging leaves. After reaching the final-hanging state the basil was watered in recovered quickly to its initial state. Contrary the chive did not indicate any lack of water. It just grew and developed more brown stalks. After watering the chive did not respond at all. \n",
        "\n",
        "It might be important to say that the chive might have been in a bad, overwatered condition straight out of the supermatket where it wad bought. The soil was quiet wet and mulded already. Also the soil seemed very muddy. \n",
        "\n",
        "To conlude, the chive might have shown better results when sowing manually in a better conditionated soil.\n",
        "\n",
        "### Diary Two\n",
        "\n",
        "The results in the previous diary led to another experiment with a different plant. Instead of using the chive for the experiment, a parsley was introduced.\n",
        "\n",
        "> Date | State of Parsley | Sate of Basil | Video Parsley | Video Basil | Notes | Time passed (h) since recovery \n",
        "> --- | --- | --- | --- | --- | --- | ---\n",
        "> | | | ------------------------------------------------------ | ------------------------------------------------------ | | \n",
        "> 16.01.20 10:30 | Newly bought plant | Newly bought plant | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie1.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum1.gif?raw=1\" height=\"100\"> |  | 0\n",
        "> 17.01.20 10:30 | No change visible | No change visible | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie2.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum2.gif?raw=1\" height=\"100\"> |  | 24\n",
        "> 17.01.20 17:30 | Leaves at the bottom start drying | Form a bit changed | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie3.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum3.gif?raw=1\" height=\"100\"> |  | 31\n",
        "> 18.01.20 10:30 | Slowly going to hang | Starts leaning sideways | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie4.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum4.gif?raw=1\" height=\"100\"> |  | 48\n",
        "> 18.01.20 21:30 | Growed a bit; stalks falling into different directions | Even more dried bottom leaves | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie5.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum5.gif?raw=1\" height=\"100\"> |  | 59\n",
        "> 19.01.20 14:30 | Same development | Por very light, needs water very soon | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie6.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum6.gif?raw=1\" height=\"100\"> |  | 76\n",
        "> 20.01.20 12:30 | Endstate: Still green no differentiatable signs of dryness | Dry bottom leaves. Hanging just a bit | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie7.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum7.gif?raw=1\" height=\"100\">  |  | 98\n",
        "\n",
        "Its very noticable how different the basil is developing this time. Instead if having its leaves hang it starts drying on the bottom where the leaves are small. The parsley on the other hand did just grow and started falling into different directions. After watering it did not show any differences just like the chives.\n",
        "\n",
        "### Findings\n",
        "Two plants of the same kinds can indeed develop completely different. This could be due to the fact that the environment is change in example temperature, moisture and so on. Also its not known under which circumstances the plants grew as they were bought from the supermarked. Each plant bought did behave different and showed different signs of dryness.\n",
        "\n",
        "For the later machine learning model, all basil footage will be combinded into theree different stages. It will be further explained in the corrisponding chapter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvILXvpTrZ9H"
      },
      "source": [
        "## 2.3. Pre-Processing\n",
        "\n",
        "To help the later machine learning processes, an image-processing pipeline was introduced. First and foremost the captured video material must be converted to single frames. Whilst having this frames already openend a few operations to improve learning can be made. Also very blurry images should be sorted out.\n",
        "\n",
        "The following diagramm is illustrating the idea behind the pipe. \n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/figures/ppipe.svg?raw=1\" height=\"380px\" style=\"margin: 3em;\">\n",
        "\n",
        "**Fig 2** - Pre Processing Pipe\n",
        "\n",
        "The figure will not be explained in detail instead the code will be executable interactively in the following. Please note that the code-snippets must be executed in the order they appear.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReUfOHov4lGs"
      },
      "source": [
        "%%bash\n",
        "# Download the Video from Server\n",
        "\n",
        "PLANTS=('basilikum' 'petersilie')\n",
        "STATES=('1' '2')\n",
        "SERVER_URL='https://video.natgo.dev/'\n",
        "\n",
        "# Create plants folder\n",
        "mkdir plants\n",
        "cd plants\n",
        "\n",
        "for plant in \"${PLANTS[@]}\";do\n",
        "  for state in \"${STATES[@]}\";do\n",
        "    wget \"$SERVER_URL$plant$state.mp4\"\n",
        "    mkdir $plant$state\n",
        "  done\n",
        "done\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AKho9NGkpv7"
      },
      "source": [
        "The snippet above will download the videos taken of the plants in two different states.\n",
        " 1. Fresh\n",
        " 2. Dry\n",
        "\n",
        "Different approaches were tried; like downloading three classes/states of each plants. But the results (which can be found in later chapters) were not that astonishing.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QtmAqyAlX9q"
      },
      "source": [
        "# imports\n",
        "import cv2 as cv2\n",
        "import numpy as np\n",
        "\n",
        "# CONSTANTS\n",
        "BLURRYNESS_THRESHOLD = 50 #@param {type: 'slider', min: 0, max: 150}\n",
        "HSV_LIGHT_GREEN = (30,100, 80)\n",
        "HSV_DARK_GREEN = (105,255,255)\n",
        "PLANT_DIR = 'plants'\n",
        "\n",
        "# FUNCTIONS #\n",
        "def variance_of_laplacian(image):\n",
        "\treturn cv2.Laplacian(image, cv2.CV_64F).var()\n",
        " \n",
        "def resize_img_to_percent(img, percent):\n",
        "  scale_percent = percent # percent of original size\n",
        "  width = int(img.shape[1] * scale_percent / 100)\n",
        "  height = int(img.shape[0] * scale_percent / 100)\n",
        "  dim = (width, height)\n",
        "  # resize image\n",
        "  resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "  return resized\n",
        "\n",
        "# Maskingfunction\n",
        "def create_green_mask(img):\n",
        "    hsv_image = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    mask = cv2.inRange(hsv_image, HSV_LIGHT_GREEN, HSV_DARK_GREEN)\n",
        "    return cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "def process_video(video, plant, state):\n",
        "  index = 0\n",
        "  while(video.isOpened()):\n",
        "    ret, frame = video.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    # Every n Frame for testing\n",
        "    if plant == 'petersilie':\n",
        "      every_n_frame = 3\n",
        "    elif plant == 'basilikum':\n",
        "      every_n_frame = 5\n",
        "    if index % every_n_frame == 0:\n",
        "      # Check for blurrynes\n",
        "      gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "      focus_measure = variance_of_laplacian(gray)\n",
        "      if focus_measure > BLURRYNESS_THRESHOLD:\n",
        "        # resize image\n",
        "        frame = resize_img_to_percent(frame, 50)\n",
        "        # mask image\n",
        "        frame = create_green_mask(frame)\n",
        "        # canny the image\n",
        "        frame = cv2.Canny(frame,100,200)\n",
        "        cv2.imwrite(f'./{PLANT_DIR}/{plant}{state}/{plant}{index}.jpg', frame)\n",
        "      else:\n",
        "        print(f'{plant} # {state} Frame #{index}: Too blurry! Focus Measure: {focus_measure}')\n",
        "    index += 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfFHrumPpPy0"
      },
      "source": [
        "This listing is showing the initial definition of functions which will be used to mask, resize or measure the focus of images. The latter one was brought up by a study from S. Pertuz, D. Puig and Miguel Ángel García in 2013 where 36 different approaches of mesuring focus were examined. The decision to use the Laplacian Variance was take because the measurement method performed best over all [1]. \n",
        "With this method, it is possible to sort out blurry images, because Videos are not always perfectly focused.\n",
        "\n",
        "In this projekt two different methods will be tried to optimize the images for the later machine learning.\n",
        "\n",
        "1. Masking the image\n",
        "2. Detecting edges on the masked image\n",
        "\n",
        "It will be investigated if these methods lead to better machine learning results. \n",
        "\n",
        "For masking the the image a coversion of the image into the HSV (Hue, Saturation, Value) takes place. The Variables `HSV_LIGHT_GREEN = (30,100, 80)\n",
        "HSV_DARK_GREEN = (105,255,255)` are defining the range in which the image should be masked. The result looks like the following image [2]:\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/basilikum_masked.jpg?raw=1\" width=400>\n",
        "  \n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/petersilie_masked.jpg?raw=1\" width=400>\n",
        "\n",
        "\n",
        "**Fig 3** - Green-masked images\n",
        "\n",
        "To further simplyfy the images another layer of simplification is made. Just using the edges found by the canny algorithm which is performing well on the masked images like seen in this picture [3]:\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/basilikum_cannied.jpg?raw=1\" width=400>\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/petersilie_cannied.jpg?raw=1\" width=400>\n",
        "\n",
        "**Fig 4** - Cannied images\n",
        "\n",
        "\n",
        "The following snippet will load the videos and passes them into the processing function to capture single frames and use the functions to modify the images. Not every frame is going to be used as the footage is shot in 60 frames per second and many frames would be nearly exactly the same.\n",
        "\n",
        "\n",
        "\n",
        "[1] https://www.semanticscholar.org/paper/Analysis-of-focus-measure-operators-for-Pertuz-Puig/8c675bf5b542b98bf81dcf70bd869ab52ab8aae9?p2df<br>\n",
        "[2] https://www.pyimagesearch.com/2014/08/04/opencv-python-color-detection/<br>\n",
        "[3] https://www.sciencedirect.com/science/article/abs/pii/S0031320300000236<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPTvi-ESN4aC"
      },
      "source": [
        "# Iterate over videos and process them\n",
        "plants = ['basilikum', 'petersilie']\n",
        "states = [1, 2]\n",
        "\n",
        "for plant in plants:\n",
        "  for state in states:\n",
        "    video = cv2.VideoCapture(f'./{PLANT_DIR}/{plant}{state}.mp4')\n",
        "    process_video(video, plant, state)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7idcZ03kpyH"
      },
      "source": [
        "## 2.4. The Robot aka the Raspberry Pi\n",
        "\n",
        "To mimic a robot a Raspbarry Pi coupled with the pi camera module was used in this project. \n",
        "\n",
        "The idea is as mentioned that the raspberry pi captures an image occasionally and predicts its state with the use of the machine learning models which will be obtained later. \n",
        "\n",
        "This is the first prototype of the raspberry-robot:\n",
        "\n",
        "<img src=\"images/pi_initial.jpg\" width=400>\n",
        "<img src=\"images/pi_initial_2.jpg\" width=400>\n",
        "\n",
        "**Fig 5** The Raspberry Pi Robot\n",
        "\n",
        "The first experiments with the pi camera module were made with the Python-library _picamera_. [4]\n",
        "\n",
        "The following snippet illustrates how pictures are taken and processed right on the Raspberry Pi (this is not executable in this notebook):\n",
        "\n",
        "```python\n",
        "#!/usr/bin/python3.7\n",
        "\n",
        "from picamera import PiCamera\n",
        "from time import sleep\n",
        "from fractions import Fraction\n",
        "import numpy as np\n",
        "import cv2\n",
        "import bot_api as bot\n",
        "\n",
        "# Globals\n",
        "image_path = '/home/pi/mediaprocessing/images/plant.jpg'\n",
        "image_masked_path = '/home/pi/mediaprocessing/images/plant_masked.jpg'\n",
        "image_canny_path = '/home/pi/mediaprocessing/images/plant_cannied.jpg'\n",
        "LG = 19\n",
        "DG = 56 \n",
        "HSV_LIGHT_GREEN = (LG,100, 100) \n",
        "HSV_DARK_GREEN = (DG,255,200)\n",
        "\n",
        "# functions\n",
        "def resize_img_to_percent(img, percent):\n",
        "  scale_percent = percent # percent of original size\n",
        "  width = int(img.shape[1] * scale_percent / 100)\n",
        "  height = int(img.shape[0] * scale_percent / 100)\n",
        "  dim = (width, height)\n",
        "  # resize image\n",
        "  resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "  return resized\n",
        "\n",
        "# Maskingfunction\n",
        "def create_green_mask(img):\n",
        "    hsv_image = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    mask = cv2.inRange(hsv_image, HSV_LIGHT_GREEN, HSV_DARK_GREEN)\n",
        "    return cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "def capture_image():\n",
        "    with PiCamera() as camera:\n",
        "        # camera set-up\n",
        "        camera.resolution = (1280, 720)\n",
        "        camera.rotation = 90\n",
        "        camera.framerate = Fraction(1, 2)\n",
        "        camera.shutter_speed = 1000000\n",
        "        camera.exposure_mode = 'off'\n",
        "        camera.iso = 100\n",
        "        sleep(5)\n",
        "        camera.capture(image_path, quality=95)\n",
        "\n",
        "capture_image()\n",
        "img = cv2.imread(image_path)\n",
        "img = resize_img_to_percent(img, 50)\n",
        "img = create_green_mask(img)\n",
        "cv2.imwrite(image_masked_path, img)\n",
        "img = cv2.Canny(img,100,200)\n",
        "cv2.imwrite(image_canny_path, img)\n",
        "```\n",
        "The image-manipulation functoins mainly stayed the same except of different HSV-values. This is necessary because the images taken by the pi camera are a bit off in colors. Also noticable is the fact that the cameras can be fully adjusted in terms of shutter-speed, exposure-time, resulotion or iso.\n",
        "So it was possible to produce medium quality pictures in difficult light situations (not direct daylight).\n",
        "\n",
        "The captured image is afterwards resized, masked and cannied. All images are saved locally to compare different machine learning models in later chapters.\n",
        "\n",
        "Here are example pictures:\n",
        "\n",
        "<img src=\"images/bad_basilikum.jpg\" width=300>\n",
        "<img src=\"images/bad_petersilie.jpg\" width=300>\n",
        "<img src=\"images/bad_petersilie_masked.jpg\" width=300>\n",
        "\n",
        "**Fig 6** - Raspberry Pi example pictures\n",
        "\n",
        "As seen in the example the quality of the images is really bad, the module might be defective or not cabable of shooting at least sharp images. As a result the masking fails as seen in Figure 6 (Even with adjusted HSV-values).\n",
        "\n",
        "So after a lot of trail and error with the pi camera module and also a logitech webcam a new approach must be taken into consideration.\n",
        "\n",
        "The next idea is to use an IP-camera for taking pictures and process the image on the raspberry pi. The solution is an app called _iPCamera - High-End NetworkCa‪m‬_ which works for iPhone and iPad. [5]\n",
        "\n",
        "<img src=\"images/ipcam_screenshot.png\" width=500>\n",
        "\n",
        "**Fig 7** - iPCamera interface in webbrowser\n",
        "\n",
        "The stream transferred over the network can be used with opencv to capture frames and process them.\n",
        "\n",
        "The former function `capture_image()` is now being completely being replaced by the following snipped:\n",
        "\n",
        "```python\n",
        "[...]\n",
        "\n",
        "IP_CAM_URL = 'http://192.168.50.194/live'\n",
        "\n",
        "def capture_image():\n",
        "  ip_video = cv2.VideoCapture(IP_CAM_URL)\n",
        "  ret, frame = ip_video.read()\n",
        "  cv2.imwrite(image_path, frame)\n",
        "  ip_video.release()\n",
        "\n",
        "[...]\n",
        "\n",
        "```\n",
        "\n",
        "With that method only one frame is captured and the quality of the images is excellent:\n",
        "\n",
        "<img src=\"images/basilikum_ipcam.jpg\" width=400>\n",
        "<img src=\"images/basilikum_ipcam_masked.jpg\" width=400>\n",
        "\n",
        "**Fig 8** - Images taken by the IP-camera\n",
        "\n",
        "This images are taken by the same camera as the training material so good results are expected.\n",
        "\n",
        "[4] https://picamera.readthedocs.io/en/release-1.13/index.html<br>\n",
        "[5] https://apps.apple.com/de/app/ipcamera-high-end-networkcam/id570912928<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8moNw8mHrTUZ"
      },
      "source": [
        "## 2.5. Telegram Bot\n",
        "\n",
        "\n",
        "To notify the plant owner this project uses the telegram-bot API. It is easy to use and no phone-verification, business-profile or similar is necessary to implement a bot like it would be needed with Whatsapp. [5][6]\n",
        "\n",
        "So after the initial bot-creation, which is easily done by messeging [_botfather_](https://core.telegram.org/bots#6-botfather) on telegram, a bot-token is generated. This token can be used to send requests to the telegram API. \n",
        "\n",
        "```python\n",
        "bot_token = 'YOUR-BOT-TOKEN'\n",
        "url = f'https://api.telegram.org/bot{bot_token}/getUpdates'\n",
        "response = requests.get(url)\n",
        "print(response.json())\n",
        "```\n",
        "\n",
        "This snippet will be used to get information about a users messeging the bot. Its needed because the API needs a user-id for sending messeges or images. \n",
        "\n",
        "Example ouput would look like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"ok\": true,\n",
        "    \"result\": [\n",
        "        {\n",
        "            \"message\": {\n",
        "                \"chat\": {\n",
        "                    \"first_name\": \"Kian\",\n",
        "                    \"id\": 388305285,\n",
        "                    \"type\": \"private\"\n",
        "                },\n",
        "                \"date\": 1613207709,\n",
        "                \"from\": {\n",
        "                    \"first_name\": \"Kian\",\n",
        "                    \"id\": 388305285,\n",
        "                    \"is_bot\": false,\n",
        "                    \"language_code\": \"en\"\n",
        "                },\n",
        "                \"message_id\": 242,\n",
        "                \"text\": \"Hey Bot!\"\n",
        "            },\n",
        "            \"update_id\": 119759391\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "The obtained user-id can be further used to send updates with plant state information. \n",
        "\n",
        "For this coursework its not necessary to implement the full-blown python telegram-api wrapper [7]. As the API is reachable over the HTTP-Protocol a simple module to send images and text would look like this:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "bot_token = 'YOUR-BOT-TOKEN'\n",
        "chat_id = '388305285' # Kians\n",
        "url = f'https://api.telegram.org/bot{bot_token}'\n",
        "\n",
        "def send_text(bot_message):\n",
        "    response = requests.get(f'{url}/sendMessage?chat_id={chat_id}&parse_mode=Markdown&text={bot_message}')\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def send_image(imageFile):\n",
        "    response = requests.post(f'{url}/sendPhoto', data={'chat_id': chat_id}, files={'photo': open(imageFile, 'rb')})\n",
        "    return response.json()\n",
        "\n",
        "```\n",
        "\n",
        "This listing can be imported into any other python-script and used to send messeges to the _chat_id_ shown in the definition scope.\n",
        "\n",
        "\n",
        "[5] https://core.telegram.org/bots<br>\n",
        "[6] https://www.facebook.com/business/m/whatsapp/business-api<br>\n",
        "[7] https://github.com/python-telegram-bot/python-telegram-bot<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXIzKYsFvIzl"
      },
      "source": [
        "## 2.6 Testing the Setup\n",
        "\n",
        "Now bringing together all the above steps some testing is neccessary to verify that everything works as intended.\n",
        "\n",
        "<img src=\"images/ipcam_setup.jpg\" width=400>\n",
        "\n",
        "**Fig 9** - IP Camera Setup\n",
        "\n",
        "For the test the image-capture snippet (in Chapter 2.4) will be execudet with the folling lines appended:\n",
        "\n",
        "```python\n",
        "bot.send_text('Here is an image for you:')\n",
        "bot.send_image(image_path)\n",
        "bot.send_image(image_masked_path)\n",
        "bot.send_image(image_canny_path)\n",
        "```\n",
        "\n",
        "The surprinsing good ouput by the bot looks like this:\n",
        "\n",
        "<img src=\"images/bot_screenshot.png\" width=400>\n",
        "\n",
        "**Fig 10** - Bot sending test images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eB8G4yjP5_q"
      },
      "source": [
        "# 3. Machine Learning\n",
        "\n",
        "## 3.1 Choosing the fitting machine learning approach \n",
        "- 3 verschiedene Ansätze\n",
        "-  warum nehmen wir Image recogn\n",
        "\n",
        "## 3.2 Definition and training of the model \n",
        "- Skizze vom Model\n",
        "- Layer dies das\n",
        "- warum relu? \n",
        "- Besonderheiten\n",
        "- trainingsprobleme? Size cropping einheitliche skalierung\n",
        "- Bilder waren zu groß\n",
        "- gerne immer paar quellen rein (auch internet-quellen wie pakete etc.)\n",
        "## 3.3 TFLite model conersation\n",
        " - h5 to tf lite how? Quelle \n",
        "## 3.4 Problems and solutions  \n",
        " - ressourcen (vorallem ram)\n",
        " - -> generator ansatz\n",
        " - overfitting? -> k-Fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGWSYt6HvH4W"
      },
      "source": []
    }
  ]
}