{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLANTOMATIC_coursework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/Conclusion/plantomatic_coursework_mediaprocessing_group_i.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp_M7EMFQTCF"
      },
      "source": [
        "**P L A N T O M A T I C** - COURSEWORK\n",
        "\n",
        "---\n",
        "\n",
        "*Kian Lütke, Carsten Montag, Johannes Maximilian Stürenburg*\n",
        "\n",
        "This document outlines the documentation and progress journal of group I of the class _Media-Processing_. This work is divided in several chapters. Each section will be presented with executable code examples which will be - if run in the correct order - in the end illustrate the whole project together. \n",
        "\n",
        "It's very important to activate the following colab features<br> Click on the gear-icon in the upper right corner and under miscellaneous/sonstiges tick the two options:\n",
        "- Corgi mode\n",
        "- Kitty mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFJa8RtR7Q8b"
      },
      "source": [
        "# 1. Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7velBbyDxDX"
      },
      "source": [
        "## 1.1. Motivation\n",
        "\n",
        "This coursework is part of a multi-project class called _Mediaprocessing_ taught by Prof. Dr. techn. Susanne Boll-Westermann and Dr.-Ing. Larbi Abdenebaoui. During the introduction weeks several Projects were introduced to pick from. As one of the group members does grow herbs at home the obvious choice here is the project _How Is My Plant Doing_. To briefly explain the goal subject: An image of a plant should be analyzed to tell whether the plant needs watering or not.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAK4eJcPD0lL"
      },
      "source": [
        "## 1.2. Case/Goal\n",
        "\n",
        "The main goal of the project is to distinguish between two different plants and classify them to get an idea about their current state. \n",
        "\n",
        "A case was developed to fund the classwork’s structure upon:\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/1-concept.png?raw=1\" height=\"300px\" style=\"margin: 3em;\">\n",
        "\n",
        "**Fig 1** - Rough Concept\n",
        "\n",
        "The idea outlines that some sort of robot will take care of a kitchen-garden. The robot should move from plant to plant taking pictures and processing the images. Afterwards the images will be analyzed and classified. The robot decides whether the plant needs to be watered or not. In the letter case a message will be send to the owner’s mobile device to inform about the plants state. The message could contain the image of the plant itself as well. As there is no access to a real robot which actually is able to move. This project will make use of a stationary Raspberry Pi computer with a camera attached to it. The Raspberry Pi will then take a set of pictures every hour and evaluate the plant-state directly.    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8EpJUczD3Na"
      },
      "source": [
        "## 1.3. Methodology\n",
        "This work is divided in two main sections: The feature analysis part and the machine learning part. The first part will cover how images were gathered and how these images will be processed in order to prepare them for the machine learning training. In the second part the actual learning of the machine learning algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKoiNLFw7zYC"
      },
      "source": [
        "# 2. Feature Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnXBdCFCEADR"
      },
      "source": [
        "\n",
        "## 2.1. Data Gathering\n",
        "\n",
        "This section describes the process of acquiring images of the plants. The first approach was to use a mirrorless system-camera to take thousands of pictures.\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/dlsm_basilikum.jpg?raw=1\" height=\"300px\" style=\"margin: 3em;\">\n",
        "\n",
        "**Fig 2** - Picture taken with a DSLM Full Frame Camera [Sony a7 – 50mm/f5.6]\n",
        "\n",
        "As seen in Figure 2 the results are astonishing. The downside comes with the amount of time needed to take all the needed images. Taking approx. 1200 images took around 3 hours. So, the idea came up to take short videos of the plant and extract single frames to process them as images. For example, a short clip of 30 seconds could generate 1800 images if every fame is used.\n",
        "\n",
        "60 FPS Footage: 30 (seconds) * 60 (fps) = 1800\n",
        "\n",
        "The drawback here is the blurriness of extracted frames. A solution will be discussed in the next section.\n",
        "\n",
        "**Choosing the right plants**\n",
        "\n",
        "At the beginning of the projects two different plants were chosen: Basil and German mint. During this project we decided – because of the difficulties to distinguish between this two plants  - to switch from mint to chives. We decided to create a diary and observe the plants in different states. More on that in the next chapter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oE5qIdzEENf"
      },
      "source": [
        "## 2.2. The Plant-Diary\n",
        "\n",
        "The first report consists of two different plants. One chive and one basil. They were captured on video in different time-intervals. The expected outcome should be a good base to start the training of later models.\n",
        "\n",
        "### Diary One\n",
        "\n",
        "> Date | State of Chive | Sate of Basil | Video Chive | Video Basil | Notes | Time passed (h) since recovery \n",
        "> --- | --- | --- | --- | --- | --- | ---\n",
        "> 09.12.20 12:30 | New | Needs watering | ------------------------------------------------------ | ------------------------------------------------------ | Last watering | n/a\n",
        "> 09.12.20 17:00 | No change visible | Fully recovered | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch1.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum1.gif?raw=1\" height=\"100\"> |  | 0\n",
        "> 10.12.20 12:10 | No change visible | No change visible | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch2.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum2.gif?raw=1\" height=\"100\"> |  | 19\n",
        "> 10.12.20 19:50 | Visible hanging of the leaves | Pot is very light, will need water soon | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch3.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum3.gif?raw=1\" height=\"100\"> |  | 27\n",
        "> 11.12.20 11:50 | Even more hanging | Leaves also start hanging; pot is very light; needs water | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch4.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum4.gif?raw=1\" height=\"100\"> |  | 43\n",
        "> 12.12.20 09:10 | Still green; growing fast | End of experiment; final state is reached | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch5.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum5.gif?raw=1\" height=\"100\"> | Basil gets watered | 64\n",
        "> 13.12.20 14:30 | Still hanging | Fully recovered again | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch6.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/basilikum6.gif?raw=1\" height=\"100\"> | Chive gets watered | 93\n",
        "> 14.12.20 12:30 | Still the same development | aborted | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch7.gif?raw=1\" height=\"100\">  | n/a |  | 116\n",
        "> 16.12.20 08:30 | Still the same development | aborted | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d1/schnittlauch8.gif?raw=1\" height=\"100\">  | n/a | Experiment aborted | 159\n",
        "\n",
        "The table shows different development between the two plants. The chive grew a lot and hat brownish stalks. The basil on the other hand indicated the lack of water with hanging leaves. After reaching the final-hanging state the basil was watered in recovered quickly to its initial state. Contrary the chive did not indicate any lack of water. It just grew and developed more brown stalks. After watering the chive did not respond at all. \n",
        "\n",
        "It might be important to say that the chive might have been in a bad, overwatered condition straight out of the supermarket where it was bought. The soil was quite wet and moldy already. Also, the soil seemed very muddy. \n",
        "\n",
        "To conclude, the chive might have shown better results when sowing manually in a better conditionated soil.\n",
        "\n",
        "### Diary Two\n",
        "\n",
        "The results in the previous diary led to another experiment with a different plant. Instead of using the chive for the experiment, a parsley was introduced.\n",
        "\n",
        "> Date | State of Parsley | Sate of Basil | Video Parsley | Video Basil | Notes | Time passed (h) since recovery \n",
        "> --- | --- | --- | --- | --- | --- | ---\n",
        "> | | | ------------------------------------------------------ | ------------------------------------------------------ | | \n",
        "> 16.01.20 10:30 | Newly bought plant | Newly bought plant | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie1.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum1.gif?raw=1\" height=\"100\"> |  | 0\n",
        "> 17.01.20 10:30 | No change visible | No change visible | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie2.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum2.gif?raw=1\" height=\"100\"> |  | 24\n",
        "> 17.01.20 17:30 | Leaves at the bottom start drying | Form a bit changed | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie3.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum3.gif?raw=1\" height=\"100\"> |  | 31\n",
        "> 18.01.20 10:30 | Slowly going to hang | Starts leaning sideways | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie4.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum4.gif?raw=1\" height=\"100\"> |  | 48\n",
        "> 18.01.20 21:30 | Growed a bit; stalks falling into different directions | Even more dried bottom leaves | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie5.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum5.gif?raw=1\" height=\"100\"> |  | 59\n",
        "> 19.01.20 14:30 | Same development | Por very light, needs water very soon | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie6.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum6.gif?raw=1\" height=\"100\"> |  | 76\n",
        "> 20.01.20 12:30 | Endstate: Still green no differentiatable signs of dryness | Dry bottom leaves. Hanging just a bit | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/petersilie7.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/gif/d2/basilikum7.gif?raw=1\" height=\"100\">  |  | 98\n",
        "\n",
        "Its very noticeable how different the basil is developing this time. Instead, if having its leaves hang it starts drying on the bottom where the leaves are small. The parsley on the other hand did just grow and started falling into different directions. After watering it did not show any differences just like the chives.\n",
        "\n",
        "### Findings\n",
        "Two plants of the same kinds can indeed develop completely different. This could be due to the fact that the environment is change in example temperature, moisture and so on. Also its not known under which circumstances the plants grew as they were bought from the supermarket. Each plant bought did behave different and showed different signs of dryness.\n",
        "\n",
        "For the later machine learning model, all basil footage will be combined into theree different stages. It will be further explained in the corresponding chapter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvILXvpTrZ9H"
      },
      "source": [
        "## 2.3. Pre-Processing\n",
        "\n",
        "To help the later machine learning processes, an image-processing pipeline was introduced. First and foremost, the captured video material must be converted to single frames. Whilst having the frames already in cached a few operations to maybe improve learning can be made. Also very blurry images should be sorted out.\n",
        "\n",
        "The following diagram is illustrating the idea behind the pipe. \n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/figures/ppipe.svg?raw=1\" height=\"380px\" style=\"margin: 3em;\">\n",
        "\n",
        "**Fig 2** - Pre Processing Pipe\n",
        "\n",
        "The figure will not be explained in detail instead the code will be executable interactively in the following. Please note that the code-snippets must be executed in the order they appear.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReUfOHov4lGs"
      },
      "source": [
        "%%bash\n",
        "# Download the Video from Server\n",
        "\n",
        "PLANTS=('basilikum' 'petersilie')\n",
        "STATES=('1' '2')\n",
        "SERVER_URL='https://video.natgo.dev/'\n",
        "\n",
        "# Create plants folder\n",
        "mkdir plants\n",
        "cd plants\n",
        "\n",
        "for plant in \"${PLANTS[@]}\";do\n",
        "  for state in \"${STATES[@]}\";do\n",
        "    wget \"$SERVER_URL$plant$state.mp4\"\n",
        "    mkdir $plant$state\n",
        "  done\n",
        "done\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AKho9NGkpv7"
      },
      "source": [
        "The snippet above will download the videos taken of the plants in two different states.\n",
        " 1. Fresh\n",
        " 2. Dry\n",
        "\n",
        "Different approaches were tried; like downloading three classes/states of each plants. But the results (which can be found in later chapters) were not that astonishing.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QtmAqyAlX9q"
      },
      "source": [
        "# imports\n",
        "import cv2 as cv2\n",
        "import numpy as np\n",
        "\n",
        "# CONSTANTS\n",
        "BLURRYNESS_THRESHOLD = 50 #@param {type: 'slider', min: 0, max: 150}\n",
        "HSV_LIGHT_GREEN = (30,100, 80)\n",
        "HSV_DARK_GREEN = (105,255,255)\n",
        "PLANT_DIR = 'plants'\n",
        "\n",
        "# FUNCTIONS #\n",
        "def variance_of_laplacian(image):\n",
        "\treturn cv2.Laplacian(image, cv2.CV_64F).var()\n",
        " \n",
        "def resize_img_to_percent(img, percent):\n",
        "  scale_percent = percent # percent of original size\n",
        "  width = int(img.shape[1] * scale_percent / 100)\n",
        "  height = int(img.shape[0] * scale_percent / 100)\n",
        "  dim = (width, height)\n",
        "  # resize image\n",
        "  resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "  return resized\n",
        "\n",
        "# Maskingfunction\n",
        "def create_green_mask(img):\n",
        "    hsv_image = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    mask = cv2.inRange(hsv_image, HSV_LIGHT_GREEN, HSV_DARK_GREEN)\n",
        "    return cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "def process_video(video, plant, state):\n",
        "  index = 0\n",
        "  while(video.isOpened()):\n",
        "    ret, frame = video.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    # Every n Frame for testing\n",
        "    if plant == 'petersilie':\n",
        "      every_n_frame = 3\n",
        "    elif plant == 'basilikum':\n",
        "      every_n_frame = 5\n",
        "    if index % every_n_frame == 0:\n",
        "      # Check for blurrynes\n",
        "      gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "      focus_measure = variance_of_laplacian(gray)\n",
        "      if focus_measure > BLURRYNESS_THRESHOLD:\n",
        "        # resize image\n",
        "        frame = resize_img_to_percent(frame, 50)\n",
        "        # mask image\n",
        "        frame = create_green_mask(frame)\n",
        "        # canny the image\n",
        "        frame = cv2.Canny(frame,100,200)\n",
        "        cv2.imwrite(f'./{PLANT_DIR}/{plant}{state}/{plant}{index}.jpg', frame)\n",
        "      else:\n",
        "        print(f'{plant} # {state} Frame #{index}: Too blurry! Focus Measure: {focus_measure}')\n",
        "    index += 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfFHrumPpPy0"
      },
      "source": [
        "This listing is showing the initial definition of functions which will be used to mask, resize or measure the focus of images. The latter one was brought up by a study from S. Pertuz, D. Puig and Miguel Ángel García in 2013 where 36 different approaches of measuring focus were examined. The decision to use the Laplacian Variance was take because the measurement method performed best overall [1]. \n",
        "With this method, it is possible to sort out blurry images, because Videos are not always perfectly focused.\n",
        "\n",
        "In this project two different methods will be tried to optimize the images for the later machine learning.\n",
        "\n",
        "1. Masking the image\n",
        "2. Detecting edges on the masked image\n",
        "\n",
        "It will be investigated if these methods lead to better machine learning results. \n",
        "\n",
        "For masking the the conversion of the image into the HSV (Hue, Saturation, Value) takes place. The Variables `HSV_LIGHT_GREEN = (30,100, 80)\n",
        "HSV_DARK_GREEN = (105,255,255)` are defining the range in which the image should be masked. The result looks like the following image [2]:\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/basilikum_masked.jpg?raw=1\" width=400>\n",
        "  \n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/petersilie_masked.jpg?raw=1\" width=400>\n",
        "\n",
        "\n",
        "**Fig 3** - Green-masked images\n",
        "\n",
        "To further simplify the images another layer of simplification is made. Just using the edges found by the canny algorithm which is performing well on the masked images like seen in this picture [3]:\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/basilikum_cannied.jpg?raw=1\" width=400>\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/petersilie_cannied.jpg?raw=1\" width=400>\n",
        "\n",
        "**Fig 4** - Cannied images\n",
        "\n",
        "\n",
        "The following snippet will load the videos and passes them into the processing function to capture single frames and use the functions to modify the images. Not every frame is going to be used as the footage is shot in 60 frames per second and many frames would be nearly exactly the same.\n",
        "\n",
        "\n",
        "\n",
        "[1] https://www.semanticscholar.org/paper/Analysis-of-focus-measure-operators-for-Pertuz-Puig/8c675bf5b542b98bf81dcf70bd869ab52ab8aae9?p2df<br>\n",
        "[2] https://www.pyimagesearch.com/2014/08/04/opencv-python-color-detection/<br>\n",
        "[3] https://www.sciencedirect.com/science/article/abs/pii/S0031320300000236<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPTvi-ESN4aC"
      },
      "source": [
        "# Iterate over videos and process them\n",
        "plants = ['basilikum', 'petersilie']\n",
        "states = [1, 2]\n",
        "\n",
        "for plant in plants:\n",
        "  for state in states:\n",
        "    video = cv2.VideoCapture(f'./{PLANT_DIR}/{plant}{state}.mp4')\n",
        "    process_video(video, plant, state)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7idcZ03kpyH"
      },
      "source": [
        "## 2.4. The Robot aka the Raspberry Pi\n",
        "\n",
        "To mimic a robot a Raspberry Pi coupled with the pi camera module was used in this project. \n",
        "\n",
        "The idea is as mentioned that the raspberry pi captures an image occasionally and predicts its state with the use of the machine learning models which will be obtained later. \n",
        "\n",
        "This is the first prototype of the raspberry-robot:\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/pi_initial.jpg?raw=1\" width=400>\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/pi_initial_2.jpg?raw=1\" width=400>\n",
        "\n",
        "**Fig 5** The Raspberry Pi Robot\n",
        "\n",
        "The first experiments with the pi camera module were made with the Python-library _picamera_. [4]\n",
        "\n",
        "The following snippet illustrates how pictures are taken and processed right on the Raspberry Pi (this is not executable in this notebook):\n",
        "\n",
        "```python\n",
        "#!/usr/bin/python3.7\n",
        "\n",
        "from picamera import PiCamera\n",
        "from time import sleep\n",
        "from fractions import Fraction\n",
        "import numpy as np\n",
        "import cv2\n",
        "import bot_api as bot\n",
        "\n",
        "# Globals\n",
        "image_path = '/home/pi/mediaprocessing/images/plant.jpg'\n",
        "image_masked_path = '/home/pi/mediaprocessing/images/plant_masked.jpg'\n",
        "image_canny_path = '/home/pi/mediaprocessing/images/plant_cannied.jpg'\n",
        "LG = 19\n",
        "DG = 56 \n",
        "HSV_LIGHT_GREEN = (LG,100, 100) \n",
        "HSV_DARK_GREEN = (DG,255,200)\n",
        "\n",
        "# functions\n",
        "def resize_img_to_percent(img, percent):\n",
        "  scale_percent = percent # percent of original size\n",
        "  width = int(img.shape[1] * scale_percent / 100)\n",
        "  height = int(img.shape[0] * scale_percent / 100)\n",
        "  dim = (width, height)\n",
        "  # resize image\n",
        "  resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "  return resized\n",
        "\n",
        "# Maskingfunction\n",
        "def create_green_mask(img):\n",
        "    hsv_image = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    mask = cv2.inRange(hsv_image, HSV_LIGHT_GREEN, HSV_DARK_GREEN)\n",
        "    return cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "def capture_image():\n",
        "    with PiCamera() as camera:\n",
        "        # camera set-up\n",
        "        camera.resolution = (1280, 720)\n",
        "        camera.rotation = 90\n",
        "        camera.framerate = Fraction(1, 2)\n",
        "        camera.shutter_speed = 1000000\n",
        "        camera.exposure_mode = 'off'\n",
        "        camera.iso = 100\n",
        "        sleep(5)\n",
        "        camera.capture(image_path, quality=95)\n",
        "\n",
        "capture_image()\n",
        "img = cv2.imread(image_path)\n",
        "img = resize_img_to_percent(img, 50)\n",
        "img = create_green_mask(img)\n",
        "cv2.imwrite(image_masked_path, img)\n",
        "img = cv2.Canny(img,100,200)\n",
        "cv2.imwrite(image_canny_path, img)\n",
        "```\n",
        "The image-manipulation functions mainly stayed the same except of different HSV-values. This is necessary because the images taken by the pi camera are a bit off in colors. Also, noticable is the fact that the cameras can be fully adjusted in terms of shutter-speed, exposure-time, resulotion or iso.\n",
        "So it was possible to produce medium quality pictures in difficult light situations (not direct daylight).\n",
        "\n",
        "The captured image is afterwards resized, masked and cannied. All images are saved locally to compare different machine learning models in later chapters.\n",
        "\n",
        "Here are example pictures:\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/bad_basilikum.jpg?raw=1\" width=300>\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/bad_petersilie.jpg?raw=1\" width=300>\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/bad_petersilie_masked.jpg?raw=1\" width=300>\n",
        "\n",
        "**Fig 6** - Raspberry Pi example pictures\n",
        "\n",
        "As seen in the example the quality of the images is really bad, the module might be defective or not capable of shooting at least sharp images. As a result, the masking fails as seen in Figure 6 (Even with adjusted HSV-values).\n",
        "\n",
        "So, after a lot of trail and error with the pi camera module and also a logitech webcam, a new approach must be taken into consideration.\n",
        "\n",
        "The next idea is to use an IP-camera for taking pictures and process the image on the raspberry pi. The solution is an app called _iPCamera - High-End NetworkCa‪m‬_ which works for iPhone and iPad. [5]\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/ipcam_screenshot.png?raw=1\" width=500>\n",
        "\n",
        "**Fig 7** - iPCamera interface in webbrowser\n",
        "\n",
        "The stream transferred over the network can be used with opencv to capture frames and process them.\n",
        "\n",
        "The former function `capture_image()` is now being completely being replaced by the following snipped:\n",
        "\n",
        "```python\n",
        "[...]\n",
        "\n",
        "IP_CAM_URL = 'http://192.168.50.194/live'\n",
        "\n",
        "def capture_image():\n",
        "  ip_video = cv2.VideoCapture(IP_CAM_URL)\n",
        "  ret, frame = ip_video.read()\n",
        "  cv2.imwrite(image_path, frame)\n",
        "  ip_video.release()\n",
        "\n",
        "[...]\n",
        "\n",
        "```\n",
        "\n",
        "With that method only one frame is captured and the quality of the images is excellent:\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/basilikum_ipcam.jpg?raw=1\" width=400>\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/basilikum_ipcam_masked.jpg?raw=1\" width=400>\n",
        "\n",
        "**Fig 8** - Images taken by the IP-camera\n",
        "\n",
        "This images are taken by the same camera as the training material so good results are expected.\n",
        "\n",
        "[4] https://picamera.readthedocs.io/en/release-1.13/index.html<br>\n",
        "[5] https://apps.apple.com/de/app/ipcamera-high-end-networkcam/id570912928<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8moNw8mHrTUZ"
      },
      "source": [
        "## 2.5. Telegram Bot\n",
        "\n",
        "\n",
        "To notify the plant owner this project uses the telegram-bot API. It is easy to use and no phone-verification, business-profile or similar is necessary to implement a bot like it would be needed with WhatsApp. [5][6]\n",
        "\n",
        "So after the initial bot-creation, which is easily done by messaging [_botfather_](https://core.telegram.org/bots#6-botfather) on telegram, a bot-token is generated. This token can be used to send requests to the telegram API. \n",
        "\n",
        "```python\n",
        "bot_token = 'YOUR-BOT-TOKEN'\n",
        "url = f'https://api.telegram.org/bot{bot_token}/getUpdates'\n",
        "response = requests.get(url)\n",
        "print(response.json())\n",
        "```\n",
        "\n",
        "This snippet will be used to get information about a users messaging the bot. Its needed because the API needs a user-id for sending messages or images. \n",
        "\n",
        "Example ouput would look like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"ok\": true,\n",
        "    \"result\": [\n",
        "        {\n",
        "            \"message\": {\n",
        "                \"chat\": {\n",
        "                    \"first_name\": \"Kian\",\n",
        "                    \"id\": 388305285,\n",
        "                    \"type\": \"private\"\n",
        "                },\n",
        "                \"date\": 1613207709,\n",
        "                \"from\": {\n",
        "                    \"first_name\": \"Kian\",\n",
        "                    \"id\": 388305285,\n",
        "                    \"is_bot\": false,\n",
        "                    \"language_code\": \"en\"\n",
        "                },\n",
        "                \"message_id\": 242,\n",
        "                \"text\": \"Hey Bot!\"\n",
        "            },\n",
        "            \"update_id\": 119759391\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "The obtained user-id can be further used to send updates with plant state information. \n",
        "\n",
        "For this coursework it's not necessary to implement the full-blown python telegram-api wrapper [7]. As the API is reachable over the HTTP-Protocol a simple module to send images and text would look like this:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "bot_token = 'YOUR-BOT-TOKEN'\n",
        "chat_id = '388305285' # Kians\n",
        "url = f'https://api.telegram.org/bot{bot_token}'\n",
        "\n",
        "def send_text(bot_message):\n",
        "    response = requests.get(f'{url}/sendMessage?chat_id={chat_id}&parse_mode=Markdown&text={bot_message}')\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def send_image(imageFile):\n",
        "    response = requests.post(f'{url}/sendPhoto', data={'chat_id': chat_id}, files={'photo': open(imageFile, 'rb')})\n",
        "    return response.json()\n",
        "\n",
        "```\n",
        "\n",
        "This listing can be imported into any other python-script and used to send messeges to the _chat_id_ shown in the definition scope.\n",
        "\n",
        "\n",
        "[5] https://core.telegram.org/bots<br>\n",
        "[6] https://www.facebook.com/business/m/whatsapp/business-api<br>\n",
        "[7] https://github.com/python-telegram-bot/python-telegram-bot<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXIzKYsFvIzl"
      },
      "source": [
        "## 2.6 Testing the Setup\n",
        "\n",
        "Now bringing together all the above steps some testing is necessary to verify that everything works as intended.\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/ipcam_setup.jpg?raw=1\" width=400>\n",
        "\n",
        "**Fig 9** - IP Camera Setup\n",
        "\n",
        "For testing the image-capture snippet (in Chapter 2.4) will be executed with the folling lines appended:\n",
        "\n",
        "```python\n",
        "bot.send_text('Here is an image for you:')\n",
        "bot.send_image(image_path)\n",
        "bot.send_image(image_masked_path)\n",
        "bot.send_image(image_canny_path)\n",
        "```\n",
        "\n",
        "The surprisingly good output by the bot looks like this:\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/kian/images/bot_screenshot.png?raw=1\" width=400>\n",
        "\n",
        "**Fig 10** - Bot sending test images\n",
        "\n",
        "The established setup looks promising and delivers high-quality images. Also, the Bot can send text and images to the owner of the plants. \n",
        "\n",
        "In the next part will cover the development of the machine-learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eB8G4yjP5_q"
      },
      "source": [
        "# 3. Machine Learning Theory\n",
        "\n",
        "Machine Learning according to Zhang (1999) is a model that uses experience to generate an algorithm that performs an automated mapping from inputs to outputs. The type of input can differ, in the case of this project the input is a picture, and the output is a classification. Machine learnings significance can be shown by comparing it to traditional programming. In the example of an e-mail spam filter, traditional programming would create a blacklist of senders that are statically classified as spam. In machine learning a model would get a set of e-mails that are classified as spam and not spam, the model itself creates a mapping method that determines if new e-mails are spam. The machine learning method that was created in this project will be referenced as the model, a model is defined by a set of hyperparameters and architectures. \n",
        "\n",
        "When comparing the Zhang’s definition of machine learning, the experience used is a set of pictures combined with the information what the picture contains. The mapping that needs to be done in the future is determining the object in a picture that has not been used for training. The model thus needs to classify an object, this is also known as a classification problem. The now described method is called supervised learning, the supervision here is giving the model said experience.\n",
        "<br />\n",
        "\n",
        "Quelle: https://www.intechopen.com/books/new-advances-in-machine-learning \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttZMaFPSF-xv"
      },
      "source": [
        "## 3.1 Approach\n",
        "\n",
        "The goal of this project is to differentiate between two different plants and then determining the state of the plant, the state in this case is the hydration. We use the supervised learning approach for image recognition in this project, we chose this approach because of the relative simplicity and the adequacy of the implementation. Other methods, like unsupervised learning where clustering techniques would be used, were examined and the final decision was made not to use unsupervides methods. \n",
        "\n",
        "To implement this learning method the model needs a dataset that includes information about the image in question, this information is supplied in the form of labelling. All programming in this project was done with Python and Keras, which is the de facto standard for machine learning implementations.\n",
        "\n",
        "</br >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUcETeWsgGiY"
      },
      "source": [
        "## 3.1 Labelling\n",
        "\n",
        "\n",
        "For the prototype of the convolutional neural network model a basic labelling method was developed. The final classes of images that will need to be classified by the neural network are shown below.\n",
        "\n",
        "1: Basil in need of watering\n",
        "\n",
        "2: Basil in good condition\n",
        "\n",
        "3: Parsley in need of watering\n",
        "\n",
        "4: Parsley in good condition\n",
        "\n",
        "These pictures were separated in different paths and labelled automatically. These labels are the additions of \"Basilikum\" and \"Parsley\" to the pictures file name. Another more sophisticated labelling process was developed which is described in Chapter 4.1.\n",
        "\n",
        "These pictures were labelled according to the object or respectively the class they show, then are used for the training and validation of the neural network. If we again look at Zhang’s definition of machine learning, this database is the experience the machine learning model will use to create a function to map the inputs. If the training is successful, the model would then get a picture of a plant it has not seen before, and then determine the state of the object recognized in this picture with a high confidence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhZv1T9LgAVg"
      },
      "source": [
        "## 3.2 Learning\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyX7LCd5iyeA"
      },
      "source": [
        "### 3.2.1 Loss Functions\n",
        "\n",
        "A neural network needs a metric to know how precise the predictions during training are, this metric is a loss or cost function and is used to change the parameters of a neural network.\n",
        "\n",
        "The binary cross entropy is used in the prototype of the convolutional neural network as the loss function. It is composed of a sigmoid activation and a cross-entropy loss and can only compute a loss with a maximum class number of two, as the name suggests a binary classification. Since the prototype of the model only needs to differentiate between two classes this function was used. It is described by the following formula.\n",
        "\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/binary_crossentropy.png?raw=1\" width=400>\n",
        "\n",
        "**Fig 11** - Crossentropy Loss Function\n",
        "\n",
        "Quelle: https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYLjeYPGltPL"
      },
      "source": [
        "This loss function can only be calculated when using supervised learning. The two relevant values in the function are the predicted class value and the actual class value, which correspond to a value of 0 and 1 in case of the binary classes. The reason the labelling of pictures is necessary is the calculation of the loss function, this way the network can determine the precision of the network by calculation the average error of all inputs.\n",
        "The result of this function is the deciding factor in changing the parameters of the network which is done automatically by the model. \n",
        "\n",
        "Quelle: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943952\n",
        "\n",
        "\n",
        "\n",
        "**Batch Size**\n",
        "\n",
        "If for example a batch size of ten is chosen for training, the loss function for these ten pictures will be calculated. The adaption of the parameters necessary to minimize the loss function will be calculated based on the loss function and the average adaption of these parameters will be applied by using the specified learning rate. This process will run iteratively to minimize the cost function of all input pictures used in training until a satisfying result is met.\n",
        "\n",
        "Quelle: https://keras.io/api/models/model_training_apis/\n",
        "\n",
        "\n",
        "**Learning Rate**\n",
        "\n",
        "The process of changing the parameters based on the cost function is called backpropagation which uses gradient descent methods.\n",
        "The goal of this process is to determine the global minimum of the loss function, this means that the error of all input pictures used in traning would be minimal. In the picture below is a two-dimensional cost function as an example, a cost function in a neural network typically is highly multidimensional and is dictated by the number of adaptable parameters in a neural network and respectively the size of the network. \n",
        "The learning rate determines how fast the changes in the parameters will be applied, some methods use a variable learning rate to counteract the below listed problems.\n",
        "\n",
        "**1:** The learning rate is too big\n",
        "This will result in the network not being able to find the global minimum because the adaption steps are too big\n",
        "\n",
        "**2:** The learning rate is too small\n",
        "This will result in the network not being able to leave a local minimum. \n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/hill_climbing_problem.png?raw=1\" width=500> \n",
        "\n",
        "**Fig 12** - Hill Climbing Problem\n",
        "\n",
        "Quelle: https://www.researchgate.net/figure/Example-of-local-and-global-solutions-in-an-optimization-problem_fig3_322270023\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZewWOCaf9j9"
      },
      "source": [
        "## 3.3 Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lV66dyDf6gl"
      },
      "source": [
        "### 3.3.1 Performance \n",
        "\n",
        "When first implementing the convolutional neural network, we used the original size of the image which was taken with a high-quality camera which resulted in problems concerning the temporary storage limitations of the hardware. Due to the multi-dimensional structure of the input data, a doubling in size of the image means an increase of the input data by the factor 4. Thus, the training is only possible with a very small batch size and even then, the network takes a considerable amount of time for training, the obvious solution was a reduction in size of the input pictures, which greatly increased the performance. Analogous to the increase of the image size, a decrease of the image size by the factor of 2 means an input size reduction by the factor 4. The question to be answered is how the final accuracy of the convolutional neural network is if the image size is greatly reduced. We decreased the size of the image so far that the training of the model is performed in an acceptable timeframe.\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtkgt5orf4Q4"
      },
      "source": [
        "### 3.3.2 Hardware\n",
        "\n",
        "Without using the generator described in Chapter **X.X**, even a small batch size will cause an error in the programming environment because the amount of temporary storage used is greater than the amount available, which already was sixteen gigabytes of RAM. This happens because all input pictures of the batch will be loaded into the ram for training, with a large amount of training data this limitation is reached very quickly.\n",
        "\n",
        "The training speed could be enhanced by using hardware that is specialized on using TensorFlow, respectively newer Nvidia hardware. This problem was solved by using the Google Collab platform which utilizes specialized hardware on a server. \n",
        "\n",
        "<br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5G7qjM2fyZ1"
      },
      "source": [
        "### 3.3.3 Generalization\n",
        "\n",
        "The goal of this convolutional neural network is the image classification of a plant and its state. A major problem is the needed generalization of the model, which is the ability to transfer acquired knowledge by the training to input data that has not been used for training, this presupposes a very large variety in the learning dataset. Since we used our own plants the variety is comparatively small, this means that the neural network could memorize the plants that we used for training instead of learning its defining features. In this case the model could only recognize the used plants reliably but not new data for predictions. This generalization can be tested by various validation methods.\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu1Xb_esfuWX"
      },
      "source": [
        "##3.4 Types of Layers in a CNN\n",
        "\n",
        "A convolutional neural network for image recognition can be composed of multiple types of layers. Typical layers are the Conv2D layer, the MaxPooling layer, a Dropout layer, a Flattening layer and a Dense layer, which will shortly be described in this chapter.\n",
        "\n",
        "\n",
        "**Conv2D Layer**\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/Conv2D.gif?raw=1\" width=500>\n",
        "\n",
        "**Fig 13** - Conv2D Layer\n",
        "\n",
        "Quelle: https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148\n",
        "\n",
        "The Input of a Conv2D layer is three-dimensional, this naming scheme can be confusing, but the two-dimensionality of this layer is based on the filter’s movement. In the example of this picture the filter has the size of three by three and a stride of one. The filter can also be called a kernel whose size defines the convolution size, a size three by three kernel convolutes nine pixels into one. The stride is the size of steps that are done after each convolution, in this animation the stride is one.\n",
        "The features of nine single pixels would be condensed into a single pixel, this produces an abstraction of a certain part of the image which could be an edge of a number or the edge of a plants part in the case of this project. Since the filter usually has overlapping due to the striding size typically being lower than the filters size, the input is not condensed linearly. The output of this layer is a set of pictures which are smaller than the input picture, this is used to get multipe abstracted sets of features of a single input picture. This output is also called a feature map. Generally speaking these subsets can represent features which could indicate the presence or abscence of a certain object. The number of features to be recognized in these maps typically is lower after each respective layer, the first feature map in the case of number recognition could represent the upper half of an eight which is a circle. Further feature maps usually are subfeatures like parts of the circle mentioned above.\n",
        "\n",
        "Quelle: https://arxiv.org/pdf/1907.12908.pdf\n",
        "\n",
        "**MaxPooling**\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/MaxPooling.gif?raw=1\" width=500 >\n",
        "\n",
        "**Fig 14** - MaxPooling Layer with size 2x2\n",
        "Quelle: https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks\n",
        "\n",
        "The MaxPooling layer depending on the pooling size takes an image array as the input, typically this input comes from the Conv2D layer. With a pooling size of two by two, the highest value of these pixels is saved and the rest discarded, this is essentially a simple downscaling of a picture which tries to maintain the essential part of the input information. If a single input consists of three black pixels and one white pixel which could represent a border of an object, only this information is saved because it is assumed that lower valued pixels are not essential to the picture’s information. This pixel value could also represent a color which may be relevant in the classification. A MaxPooling size of two by two would reduce the input by the factor 4. The goal of this layer is the reduction of the input size with the minimal loss of valuable information, which will increase performance.\n",
        "\n",
        "Quelle: https://arxiv.org/pdf/1907.12908.pdf\n",
        "\n",
        "**Dropout Layer**\n",
        "\n",
        "This layer’s function is to stop overfitting in the neural network, which would mean the network is not generalizing well and learns the inputs used for training. This is especially prevalent in small datasets.\n",
        "Parts of the input are then forgotten at a random rate which according to Srivastava et. Al. prevents units from co-adapting too much, which improved performance in multiple supervised learning tasks.\n",
        "\n",
        "Quelle: https://jmlr.org/papers/v15/srivastava14a.html\n",
        "\n",
        "\n",
        "**Flattening**\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/Flattening.png?raw=1\" width=500>\n",
        "\n",
        "**Fig 15** - Flattening Step\n",
        "\n",
        "Quelle: https://www.superdatascience.com/blogs/convolutional-neural-networks-cnn-step-3-flattening\n",
        "\n",
        "The Flattening layer is a preparation for the further input of the fully connected layer, this fully connected layer can be described as a traditional non-convolutional neural network. This input needs to be a single array due to the nature of the used network; the flattening takes all multidimensional feature subsets and produces one one-dimensional array. This step does not condense information and is solely a data transformation.\n",
        "\n",
        "\n",
        "\n",
        "**Fully Connected Layer**\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/fully_connected.png?raw=1\" width=500>\n",
        "\n",
        "**Fig 16** - Fully Connected Layer \n",
        "\n",
        "\n",
        "Quelle: https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-convolutional-neural-network-3607be47480\n",
        "\n",
        "\n",
        "This layer takes the array of values from the flattening step and is composed of multiple layers of classical neurons. The Conv2D, MaxPooling and Flattening steps before this layer can be seen as preparations for the majority of calculations the model needs to do in order for a prediction. As described in the learning chapter these weights and biases of the neurons in this layer are adapted to improve the model’s prediction capabilities.\n",
        "\n",
        "\n",
        "**Dense Layer**\n",
        "\n",
        "The Dense layer is a condensation of a previous layer, because of this reduction of the neuron count this is called the Dense layer. If the goal of a CNN is the determination if the object in a picture is a dog or a cat, the last layer should be composed of two neurons in a dense layer. One of these neurons is symbolically a cat and the other a dog, if the final value of the neuron classifying a cat is 0.84, this would mean that the object in said picture is a cat with 84% certainty. The number of dense neurons in the last layer is typically the number of possible classes that should be predicted by a convolutional neural network.\n",
        "[QUELLE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nlyX14zfdVw"
      },
      "source": [
        "## 3.5 Custom prototype model\n",
        "\n",
        "For the first approach a custom model was created using a set of all layers mentioned before.\n",
        "<br />\n",
        "\n",
        "```python\n",
        "def get_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(IMG_HEIGHT,IMG_WIDTH,COLOR)))\n",
        "    model.add(layers.MaxPooling2D((2,2)))\n",
        "    model.add(layers.Conv2D(64, (3,3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D((2,2)))\n",
        "    model.add(layers.Conv2D(128, (3,3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D((2,2)))\n",
        "    model.add(layers.Conv2D(128, (3,3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D((2,2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dense(NUMBER_OF_CLASSES, activation=\"softmax\"))\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\"acc\"])\n",
        "    return model\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0yq-6gMO8Fm"
      },
      "source": [
        "The models layers and hyperparameters were added in this function and the ReLU activation function was used. This function does not change positive values and negative values are mapped to zero, as seen in the figure below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCBuNmTVfDZl"
      },
      "source": [
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/relu.png?raw=1\" width=300>\n",
        "\n",
        "**Fig 17** - ReLU Activationfunction\n",
        "\n",
        "Quelle: https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b03blXcGP5G"
      },
      "source": [
        "**Visualization of Layer Outputs**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6y8_l_xGTou"
      },
      "source": [
        "The following figure is the input of the CNN with the shape of 960x540x3. 960 being the width, 540 the height and 3 the color channels. The brightness of the pictures pixels indicates the activation function of each pixel, the scale of this activation value is represented in the bar at the right of the figure.\n",
        "<br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EYcFVCoD7Bh"
      },
      "source": [
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/1.png?raw=1\" width=700>\n",
        "\n",
        "**Fig 18** - Input Shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngSjboSPHS3h"
      },
      "source": [
        "The following picture is the result of multiple MaxPooling and Conv2D layers. These pictures are feature subsets, visually the most prominent features are the edges of the plants leaves.\n",
        "\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmMZiPR_Gq2W"
      },
      "source": [
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/4.png?raw=1\" width=1700>\n",
        "\n",
        "**Fig 19** - Shape after multiple convolution and pooling steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7Tm7a7XIk9y"
      },
      "source": [
        "These subsets are then flattened into a single array which is then used in the fully connected layer. The following picture is the single array with the corresponding pixels which function as the input for the fully connected layer. This array is scaled and composed of numbers between zero and one which enable a neural network to process them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkLj43MeLTAK"
      },
      "source": [
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/7.png?raw=1\" width=1700>\n",
        "\n",
        "**Fig 20** - Output of the flattening process (Exerpt)\n",
        "\n",
        "The last layer of the convolutional neural network represents the number of classes which are predicted by the network, in this case the last layer consists of two dense neurons representing two plants. The right neuron contains the value zero, and the left neuron, which is white the value one, which means that the network classifies the input picture to the left neuron with a certainty of 100% or 1.0.\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/6.png?raw=1\" width=1700>\n",
        "\n",
        "**Fig 21** The last layer showing two neurons for the classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsBvFPGrvH72"
      },
      "source": [
        "Below is the summary of the custom model, the layers and parameters of each layers are listed here. A total of 118,076,098 parameters can be adapted to minimize the cost function. This translates to a global cost function with more than a hundred million dimensions, this resulted in high time and hardware demands for training and paired with non-optimal results led to the rejection of the custom model in favor of the EfficientNet architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbCxH9ANiFd"
      },
      "source": [
        "```Python\n",
        "Layer (type)                 Output Shape              Param #\n",
        "=================================================================\n",
        "conv2d (Conv2D)              (None, 538, 958, 32)      896\n",
        "_________________________________________________________________\n",
        "max_pooling2d (MaxPooling2D) (None, 269, 479, 32)      0\n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 267, 477, 64)      18496\n",
        "_________________________________________________________________\n",
        "max_pooling2d_1 (MaxPooling2 (None, 133, 238, 64)      0\n",
        "_________________________________________________________________\n",
        "conv2d_2 (Conv2D)            (None, 131, 236, 128)     73856\n",
        "_________________________________________________________________\n",
        "max_pooling2d_2 (MaxPooling2 (None, 65, 118, 128)      0\n",
        "_________________________________________________________________\n",
        "conv2d_3 (Conv2D)            (None, 63, 116, 128)      147584\n",
        "_________________________________________________________________\n",
        "max_pooling2d_3 (MaxPooling2 (None, 31, 58, 128)       0\n",
        "_________________________________________________________________\n",
        "flatten (Flatten)            (None, 230144)            0\n",
        "_________________________________________________________________\n",
        "dropout (Dropout)            (None, 230144)            0\n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 512)               117834240\n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)              (None, 2)                 1026\n",
        "=================================================================\n",
        "Total params: 118,076,098\n",
        "Trainable params: 118,076,098\n",
        "Non-trainable params: 0\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho8Jw6Km5kgp"
      },
      "source": [
        "## 3.6 EfficientNet\n",
        "\n",
        "The EfficientNet was presented in 2019 by Tan Mingxingand and V. Quoc in the Paper *EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks*\n",
        "\n",
        "Through repeated scaling this net has state-of-the-art performance and precision in image recognition. There are multiple ways of scaling a convolutional neural network to improve its performance which are visualized in the figure below. \n",
        "\n",
        "**a:** increasing neural network input resolution\n",
        "\n",
        "**b:** increasing neural network width\n",
        "\n",
        "**c:** increasing neural network depth\n",
        "\n",
        "The scaling of these parameters used to be done arbitrarily, the solution of the EfficientNet is to provide a ratio in what ratios these scaling methods are performed in the form of compound scaling, respectively the scaling of all three methods shown.\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/effinet_architecture.png?raw=1\" width=700>\n",
        "\n",
        "**Fig 20** - EfficientNet Architecture\n",
        "\n",
        "Quelle: https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\n",
        "\n",
        "The Baseline of a model can be seen in part a of the upper figure, this is the model that needs to be changed in order to improve either performance, precision or both. In the figure below the accuracy relative to the FLOPS is shown, FLOPS are floating point operations per second, this is a metric for the computational performance. The EfficientNet Architecture can be scaled into categories B0-7, each category having a higher compound scaling, thus more accuracy and more FLOPS needed. The EfficientNet has better accuracy in the ImageNet database than every other convolutional neural network architecture it was compared to. When this paper was reviewed, the decision was made to use the EfficientNet architecture for further implementation of the project after the custom model was discarded.\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/effinet_efficiency.png?raw=1\" width=700>\n",
        "\n",
        "**Fig 20** - EfficientNet Efficiency\n",
        "\n",
        "\n",
        "\n",
        "QUELLE: https://arxiv.org/pdf/1905.11946.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlE8WQFMjScF"
      },
      "source": [
        "# 4. Machine Learning in Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJJl776yihcI"
      },
      "source": [
        "## 4.1. Preparations for model training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5J5uHbsiZlk"
      },
      "source": [
        "\n",
        "### 4.1.1 Labeling the data\n",
        "\n",
        "Because for the machine learning model a supervised learning approach was chosen, the pictures generated and cut from the plant-diary videos by the preprocessing pipeline had to be labeled in order to feed them to the model.\n",
        "\n",
        "The neural net after training shall be able to differentiate and classify dependent on model performance between two to three states for each plant recorded in our diary. For that the produced videos were fit into three categories to later attach the corresponding label to the filename. These labels are \"healthy\", \"in need of water\" and \"dried up\" for each plant. Because over the duration of the project the plants to classify were changed a few times, so the labeling process had to be designed accordingly variable.\n",
        "In order to do that, the recorded videos were named matching the given plants species and with a rising number matching the state of the plant. So a video named \"basil1\" would be a video of a fresh from-store basil plant. The worse the state of the plant the higher the number following the plant name.\n",
        "With all of that in mind the machine learning programming process began with conceiving a class based label generator for the pipeline which later was refactored to produce the K Folds.\n",
        "The following code snippet shows the constructor of the class which will be explained in the text block after.\n",
        "\n",
        "```python\n",
        "#initializing the directorys and label dictionaries \n",
        "class KFoldGenerator():\n",
        "  def __init__(self,data_dir,plants):\n",
        "      self.RS = 69\n",
        "      random.seed = self.RS\n",
        "      self.plants = plants\n",
        "      self.data_dir = data_dir\n",
        "      self.nr_classes = len(plants)*3 \n",
        "      self.dir_list = [dir for dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,dir))]\n",
        "      if '.ipynb_checkpoints' in self.dir_list : \n",
        "        self.dir_list.remove('.ipynb_checkpoints')\n",
        "      self.plant_nrs = []\n",
        "      for plant in plants:\n",
        "          nrs = sorted([int(dir.replace(plant,'')) for dir in self.dir_list if plant in dir])\n",
        "          self.plant_nrs.append(nrs)\n",
        "      self.class_dict = {}\n",
        "      for i,plant in enumerate(self.plants):\n",
        "          nrs = self.plant_nrs[i]\n",
        "          self.class_dict.update({\n",
        "            f'{plant}{nrs[0]}' : f'_class_{plant}_frisch',\n",
        "            f'{plant}{nrs[1]}' : f'_class_{plant}_giessen',\n",
        "            f'{plant}{nrs[2]}' : f'_class_{plant}_vertrocknet',\n",
        "          })\n",
        "      \n",
        "      filepaths = self.__load_paths__()\n",
        "      self.filepaths = self.__set_class__(filepaths)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F7GGEtlipFF"
      },
      "source": [
        "The constructor takes an array of plant names to classify into its two to three states as strings together with the parent folder path in which the subfolders for the different classes are located. \r\n",
        "\r\n",
        "These sub-folders already contain the cropped images generated from the videos. \r\n",
        "Based on the number of plants given in the \"plants\" array, the number of classes for the neural network for classifying is set by multiplying the length of the array by three.\r\n",
        "\r\n",
        "Following that a list comprehension determines the sub folder paths contained in the parent directory. The subdirectory names which are the base for each class are now stored in the class variable \"dir_list\".\r\n",
        "\r\n",
        "Based on that the numbers following the plant name in the directory name had to be ordered to allow for a dictionary to be created which is intended to be used to match a label to each given picture in those subdirectories. This is done in the for-loop beginning in line 12.\r\n",
        "\r\n",
        "The loop iterates over the in the constructor passed string array to determine the corresponding directory numbers emerging from its video name for each label and plant ordered by plant state. The determined numbers for each plant are now used to create a dictionary with the plant name and directory number as key and the class as value to be used in the actual labelling process. This allows us to pick later diary entries with higher video numbers for our neural net because with this approach the pipeline is able to treat [\"basil1.mp4\", \"basil2.mp4\", \"basil3.mp4\"] the same way it treats [\"basil2.mp4\", \"basil5.mp4\", \"basil8.mp4\"] without us being forced to manually change anything in the pipeline except the videos to generate cropped pictures from in the short bash script at the beginning of the pipeline used for file downloading. \r\n",
        "\r\n",
        "\r\n",
        "Also the number of states to classify between is easy to change by just configuring the classes dictionary and number of videos served to the pipeline.\r\n",
        "With all required preparations taken care of the actual labeling process is called in the last two lines of the __ init __() function.\r\n",
        "The function __ load_paths __() takes care of collecting the complete file paths for each picture in the targeted subdirectories into a single list and return it.\r\n",
        "\r\n",
        "```python\r\n",
        "#load all filepaths\r\n",
        "def __load_paths__(self):\r\n",
        "      all_paths = []\r\n",
        "      for dir in self.dir_list:\r\n",
        "         dir_path = os.path.join(self.data_dir,dir) \r\n",
        "         paths = [os.path.join(dir_path,file) for file in os.listdir(dir_path)]\r\n",
        "         all_paths.extend(paths)\r\n",
        "      return all_paths \r\n",
        "      \r\n",
        "``` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdfYf7IF1_Zf"
      },
      "source": [
        "Now the labels can be set which is done in the __ set_class __() function by applying the previously generated dictionary to each file path in the \"filepaths\" list.\r\n",
        "\r\n",
        "```python\r\n",
        "#define the class based on the src dir\r\n",
        "def __set_class__(self,files):\r\n",
        "    classes = []\r\n",
        "    for file in files:\r\n",
        "      dir = os.path.dirname(file)\r\n",
        "      self.class_dict.get(os.path.basename(os.path.normpath(dir)))\r\n",
        "      tup = (file,self.class_dict.get(os.path.basename(os.path.normpath(dir))))\r\n",
        "      classes.append(tup) \r\n",
        "    random.shuffle(classes)\r\n",
        "    return classes\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfQ8S6Eh2gHI"
      },
      "source": [
        "To do that the function iterates over each image path, isolates the subdirectory path the image is stored in as a single string without special characters to match the dictionary keys. The resulting classes returned by the get() function of the dictionary are stored together with the complete file path as a tuple which leaves us with a list of labels and image paths to be used for a single model training as well as for the KFold generation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V_UFYwucFEQ"
      },
      "source": [
        "### 4.1.2 A KFold cross validation for performance measuring\n",
        "To determine the performance of a given model it has to be validated what means testing it on different test data sets. In Image Recognition in contrast to for example Time Series Analysis a simple so called kfold cross validation is the standard to validate the performance of a machine learning model.\n",
        "\n",
        "Performing a kfold cross validation means splitting the given dataset into k   splits with train and test data while each split contains 1/k of the original dataset as test set for validation. The remaining data is used for training. By that each picture of the complete data set is exactly once used for validation and with calculating the mean performance by multiplying each result with 1/k and adding them up afterwards a well validated performance measure in form of the validation accuracy is the result[1]. \n",
        "\n",
        "The following picture illustrates the concept very good. \n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/kfold_illustration.png?raw=1\" width=800>[2]\n",
        "\n",
        "**Fig 21** - K-Fold Illustration\n",
        "\n",
        "To implement this the label generator class had to be extended with functions serving the purpose of generating the folds and copying the cropped pictures\n",
        "to newly created folders for each validation step.\n",
        "\n",
        "[1] Aurélien Géron, \"Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems\",O´Reilly Media,2019\n",
        "\n",
        "[2] https://miro.medium.com/max/4984/1*kheTr2G_BIB6S4UnUhFp8g.png\n",
        "\n",
        "\n",
        "```python\n",
        "#generate all folds\n",
        "def generate_k_folds(self,k):\n",
        "    copy_list = self.filepaths[:]\n",
        "    test_size = floor(len(self.filepaths)/k)\n",
        "    rest = len(self.filepaths)%k\n",
        "    folds = []\n",
        "    for i in range(0,k):\n",
        "      if i == k-1:\n",
        "        test_size = test_size + rest\n",
        "      folds.append(self.__generate_fold__(copy_list,test_size))\n",
        "    dir_kfolds = 'K_FOLDS'  \n",
        "    os.mkdir('K_FOLDS')\n",
        "    for i,fold in enumerate(folds):\n",
        "      current_fold = f'{dir_kfolds}/Fold{i+1}'\n",
        "      train = fold[0]\n",
        "      test = fold[1]\n",
        "      print(test)  \n",
        "      os.mkdir(current_fold)\n",
        "      test_dir = f'{current_fold}/test'\n",
        "      train_dir = f'{current_fold}/train'\n",
        "      os.mkdir(test_dir)\n",
        "      os.mkdir(train_dir)  \n",
        "      self.__copy_list_to_dir__(test,test_dir)\n",
        "      self.__copy_list_to_dir__(train,train_dir)\n",
        "#generate a single fold\n",
        "  def __generate_fold__(self,liste,size):\n",
        "    test = []\n",
        "    while len(test)<size:\n",
        "      test_item = random.choice(liste)\n",
        "      test.append(test_item)\n",
        "      liste.remove(test_item)\n",
        "    train = [file for file in self.filepaths if file not in test ]\n",
        "    return (train, test)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPJ6FlSPI2Df"
      },
      "source": [
        "The public method generate_k_folds() of the generator has to be called in order to generate the splits and copy them directly into the working directory. \r\n",
        "\r\n",
        "This is done by generating a copy of the in the constructor already labeled pictures, calculating the size for each split also taking the slightly bigger last split into consideration by calculating its overhead. \r\n",
        "\r\n",
        "In the for loop the function __ generate_fold __() is called k times each time taking 1/k*dataset_size random pictures from the list adding them to a new list. The original list entries are removed from the copied list to make sure they are not taken into account in the next iteration. \r\n",
        "\r\n",
        "After generating the test set in the function the matching train set is generated by using the concept of list comprehension copying each file path with its label not present in the test set.\r\n",
        "\r\n",
        "The function returns a tuple containing the train set on the first index and the test set on the second index. These tuples are added to the list folds which is iterated over again beginning in line 11 to copy the files according to the generated folds.\r\n",
        "\r\n",
        "For that purpose for each fold generated a folder is created with the name Fold_i containingg two subfolders train and test. \r\n",
        "At the end of the public function the generated train and test lists are separately handed to the __ copy_list_to_dir __() function together with the previously in the for loop created directory paths.\r\n",
        "\r\n",
        "```python\r\n",
        "def __copy_list_to_dir__(self,list,dir):  \r\n",
        "    for item in list:\r\n",
        "      #read src path and label for each image \r\n",
        "      src = item[0]\r\n",
        "      label = item[1]\r\n",
        "      #gather file infos\r\n",
        "      file_info = os.path.splitext(os.path.basename(src))\r\n",
        "      #generate and copy to new filepath based of the given dir\r\n",
        "      dst = f'{dir}/{file_info[0]}{label}{file_info[1]}'\r\n",
        "      copy2(src = src,dst = dst)\r\n",
        "\r\n",
        "``` \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM6ZgvflPtUD"
      },
      "source": [
        "This function uses the copy2 function of the previously imported shutil package to copy a single file from a given source location handed over in the list of tuples to a destination location. \r\n",
        "\r\n",
        "This is generated for each picture in the list in line five by putting together the handed over arguments in the list of tuples. The new file path is created by concatenating the original raw filename with the new folder path and the generated label in a formatted string. This label can later be interpreted by the training algorithm.\r\n",
        "\r\n",
        "The result of executing the function is a directory containing all generated folds called \"K_FOLDS\" containing a train and a test folder for each training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6HG0t7tU1Q_"
      },
      "source": [
        "K_FOLDS = 10\r\n",
        "PLANTS = ['basilikum', 'petersilie']\r\n",
        "DIR ='plants'\r\n",
        "gen = KFoldGenerator(DIR,PLANTS)\r\n",
        "gen.generate_k_folds(K_FOLDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7S9N_oZU82z"
      },
      "source": [
        "So with the number of folds set to 10 the resulting folder produced by calling the function in line five of the above code block would look as follows : \n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/folds.jpg?raw=1\" width=200>\n",
        "\n",
        "**Fig 22** - K-Fold Structure\n",
        "\n",
        "The following executable code block contains the constructor call of the class together with the generate_k_folds() call below. This performs the complete fold generation process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS0Wna46HYNN"
      },
      "source": [
        "import json\r\n",
        "import os\r\n",
        "import random\r\n",
        "from math import floor  \r\n",
        "from shutil import copy2 \r\n",
        "\r\n",
        "class KFoldGenerator():\r\n",
        "  def __init__(self,data_dir,plants):\r\n",
        "      self.RS = 96\r\n",
        "      random.seed = self.RS\r\n",
        "      self.plants = plants\r\n",
        "      self.data_dir = data_dir\r\n",
        "      self.nr_classes = len(plants)*3 \r\n",
        "      self.dir_list = [dir for dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,dir))]\r\n",
        "      if '.ipynb_checkpoints' in self.dir_list : \r\n",
        "        self.dir_list.remove('.ipynb_checkpoints')\r\n",
        "      self.plant_nrs = []\r\n",
        "      for plant in plants:\r\n",
        "          nrs = sorted([int(dir.replace(plant,'')) for dir in self.dir_list if plant in dir])\r\n",
        "          self.plant_nrs.append(nrs)\r\n",
        "      self.class_dict = {}\r\n",
        "      for i,plant in enumerate(self.plants):\r\n",
        "          nrs = self.plant_nrs[i]\r\n",
        "          self.class_dict.update({\r\n",
        "            f'{plant}{nrs[0]}' : f'_class_{plant}_frisch',\r\n",
        "            f'{plant}{nrs[1]}' : f'_class_{plant}_trocken',\r\n",
        "          })\r\n",
        "      \r\n",
        "      filepaths = self.__load_paths__()\r\n",
        "      self.filepaths = self.__set_class__(filepaths)\r\n",
        "      \r\n",
        "  def __load_paths__(self):\r\n",
        "      all_paths = []\r\n",
        "      for dir in self.dir_list:\r\n",
        "         dir_path = os.path.join(self.data_dir,dir) \r\n",
        "         paths = [os.path.join(dir_path,file) for file in os.listdir(dir_path)]\r\n",
        "         all_paths.extend(paths)\r\n",
        "      return all_paths \r\n",
        "  \r\n",
        "  def __set_class__(self,files):\r\n",
        "    classes = []\r\n",
        "    for file in files:\r\n",
        "      dir = os.path.dirname(file)\r\n",
        "      self.class_dict.get(os.path.basename(os.path.normpath(dir)))\r\n",
        "      tup = (file,self.class_dict.get(os.path.basename(os.path.normpath(dir))))\r\n",
        "      classes.append(tup) \r\n",
        "    random.shuffle(classes)\r\n",
        "    return classes\r\n",
        "      \r\n",
        "  def generate_k_folds(self,k):\r\n",
        "    copy_list = self.filepaths[:]\r\n",
        "    test_size = floor(len(self.filepaths)/k)\r\n",
        "    rest = len(self.filepaths)%k\r\n",
        "    folds = []\r\n",
        "    for i in range(0,k):\r\n",
        "      if i == k-1:\r\n",
        "        test_size = test_size + rest\r\n",
        "      folds.append(self.__generate_fold__(copy_list,test_size))\r\n",
        "    dir_kfolds = 'K_FOLDS'  \r\n",
        "    os.mkdir('K_FOLDS')\r\n",
        "    for i,fold in enumerate(folds):\r\n",
        "      current_fold = f'{dir_kfolds}/Fold{i+1}'\r\n",
        "      train = fold[0]\r\n",
        "      test = fold[1]\r\n",
        "      print(test)  \r\n",
        "      os.mkdir(current_fold)\r\n",
        "      test_dir = f'{current_fold}/test'\r\n",
        "      train_dir = f'{current_fold}/train'\r\n",
        "      os.mkdir(test_dir)\r\n",
        "      os.mkdir(train_dir)  \r\n",
        "      self.__copy_list_to_dir__(test,test_dir)\r\n",
        "      self.__copy_list_to_dir__(train,train_dir)\r\n",
        "\r\n",
        "  def __generate_fold__(self,liste,size):\r\n",
        "    test = []\r\n",
        "    while len(test)<size:\r\n",
        "      test_item = random.choice(liste)\r\n",
        "      test.append(test_item)\r\n",
        "      liste.remove(test_item)\r\n",
        "    train = [file for file in self.filepaths if file not in test ]\r\n",
        "    return (train, test)\r\n",
        "\r\n",
        "  def __copy_list_to_dir__(self,list,dir):  \r\n",
        "    print(dir)\r\n",
        "    for item in list:\r\n",
        "      src = item[0]\r\n",
        "      label = item[1]\r\n",
        "      file_info = os.path.splitext(os.path.basename(src))\r\n",
        "      dst = f'{dir}/{file_info[0]}{label}{file_info[1]}'\r\n",
        "      copy2(src = src,dst = dst)\r\n",
        "\r\n",
        "\r\n",
        "K_FOLDS = 10\r\n",
        "PLANTS = ['basilikum', 'petersilie']\r\n",
        "DIR ='plants'\r\n",
        "gen = KFoldGenerator(DIR,PLANTS)\r\n",
        "gen.generate_k_folds(K_FOLDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZD3NA51IhFI"
      },
      "source": [
        "## 4.2. Training the models \r\n",
        "In order to train the models with the generated datasets the folders containing the images have to be placed in memory. \r\n",
        "This is done by calling the function listdir in the native python package \"os\".\r\n",
        "The function returns all folders and files for a given directory path as strings in a non ordered fashion. \r\n",
        "```python\r\n",
        "K_DIR = 'K_FOLDS'\r\n",
        "RES_DIR = 'results'\r\n",
        "NUM_LABELS = len(gen.class_dict)\r\n",
        "dict_values = list(gen.class_dict.values())\r\n",
        "folds  = os.listdir(K_DIR) \r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncvd6chMZZ03"
      },
      "source": [
        "So the list returned in line five with the previously generated folds shown in the picture in chapter 3.2.2 would contain the strings ['Fold1','Fold10','Fold2','Fold3','Fold4','Fold5','Fold6','Fold7','Fold8','Fold9']. This list has to be ordered by the number of the fold to train the models in the correct order. This is realized by ordering the list with the utility function natural_keys() using regular expressions shown below. \r\n",
        "```python\r\n",
        "def atoi(text):\r\n",
        "    return int(text) if text.isdigit() else text\r\n",
        "def natural_keys(text):\r\n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\r\n",
        "sorted = folds.sort(key = natural_keys) \r\n",
        "```\r\n",
        "\r\n",
        "With the ordered subdirectories representing each fold representing the data in memory as a next step the Image Recognition models had to be defined.\r\n",
        "\r\n",
        "The executable code block below performs later needed import statements as well as the described fold sorting process.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWsui83wJb-v"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import models\r\n",
        "from tensorflow.keras import optimizers\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.preprocessing.image import (array_to_img, img_to_array,\r\n",
        "                                                  load_img)\r\n",
        "from tensorflow.keras.applications import EfficientNetB1\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers.experimental import preprocessing\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# defining the image shapes\r\n",
        "IMG_H = 240\r\n",
        "IMG_W = 240\r\n",
        "IMG_C = 3\r\n",
        "\r\n",
        "K_DIR = 'K_FOLDS'\r\n",
        "RES_DIR = 'results'\r\n",
        "NUM_LABELS = len(gen.class_dict)\r\n",
        "dict_values = list(gen.class_dict.values())\r\n",
        "print(NUM_LABELS,dict_values)\r\n",
        "folds  = os.listdir(K_DIR) \r\n",
        "\r\n",
        "def atoi(text):\r\n",
        "    return int(text) if text.isdigit() else text\r\n",
        "def natural_keys(text):\r\n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\r\n",
        "\r\n",
        "sorted = folds.sort(key = natural_keys) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqut3M51ieFa"
      },
      "source": [
        "\r\n",
        "### 4.2.1 Defining the model\r\n",
        "For the neural network a self configured model consisting of some Conv2D layers always followed by a MaxPooling2D layer was tested first. \r\n",
        "\r\n",
        "The Output for each tested model is generated using a Dense layer as output containing an output node for each assigned label. In this case a Flatten layer is included between the basic structure and the Output nodes. \r\n",
        "\r\n",
        "If an image is shown to the network, while or after training, the model outputs a percentage value for each class. The node with the highest value represents the class the neural network predicted for this picture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp6OHk1Tvoj0"
      },
      "source": [
        "\r\n",
        "The network is trained by serving it the pixels as a numpy array together with the desired label also as a numpy array as input. The network trains by adjusting its weights each time a batch of pictures is served. This is done according to the assigned loss function which in this case is the categorical crossentropy. It measures the error between the nets predictions and the actual label for each batch of images shown. \r\n",
        "This value together with the loss function at the end of each epoch of the training process defines the amount each weight in the neural network has to be changed. By that a high loss value results in a greater change of weights than a low loss value does.\r\n",
        "To make sure the neural network doesn´t just learn the images by heart the network is shown the test dataset which it hasn´t been trained on each epoch. By calculating the accuracy for the test dataset the model performance is measured.\r\n",
        "\r\n",
        "For each of the different models tested a separate function returning the compiled model was created. \r\n",
        "\r\n",
        "The definition for the originally used basic model is shown in the code block below. The separate layers are imported from the keras.tensorflow.layers library.\r\n",
        "```python\r\n",
        "def get_model_custom():\r\n",
        "    model = models.Sequential()\r\n",
        "    model.add(layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(IMG_H,IMG_W,IMG_C)))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Conv2D(64, (3,3), activation=\"relu\"))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Conv2D(128, (3,3), activation=\"relu\"))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Conv2D(128, (3,3), activation=\"relu\"))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Flatten())\r\n",
        "    model.add(layers.Dropout(0.5))\r\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\r\n",
        "    model.add(layers.Dense(NUM_LABELS, activation=\"sigmoid\"))\r\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\"acc\"])\r\n",
        "    return model\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rgmW8mI0KPz"
      },
      "source": [
        "The input shape and therefore the required shape of the images for the network is defined in the layer definition of the networks first layer. \r\n",
        "The last layer takes the number of labels as input to define how many dense nodes have to be added the last dense layer.\r\n",
        "The returned model is ready for training.\r\n",
        "\r\n",
        "Later because the model didn´t perform well, a well established pre trained EfficientNet was used for further training.\r\n",
        "\r\n",
        "Because the model is pretrained the model definition is a bit more complicated but effectively has the same components as the self made model. Also, the output is the compiled model which is ready to train. This is useful because it takes only one line of code to change the desired model.\r\n",
        "\r\n",
        "```python\r\n",
        "  def get_model_Effi():\r\n",
        "\r\n",
        "  # Adam \r\n",
        "  adam = tf.keras.optimizers.Adam(\r\n",
        "    learning_rate=0.0005,\r\n",
        "    name='Adam')\r\n",
        "  img_augmentation = Sequential(\r\n",
        "    [\r\n",
        "        preprocessing.RandomRotation(factor=0.15),\r\n",
        "        preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\r\n",
        "        preprocessing.RandomFlip(),\r\n",
        "        preprocessing.RandomContrast(factor=0.1),\r\n",
        "    ],\r\n",
        "    name=\"img_augmentation\",\r\n",
        "  )\r\n",
        "  NUM_CLASSES = 4\r\n",
        "  inputs = layers.Input(shape=(IMG_H, IMG_W, 3))\r\n",
        "  x = img_augmentation(inputs)\r\n",
        "  outputs = EfficientNetB1(include_top=True, weights=None, classes=NUM_CLASSES(x)\r\n",
        "  model = tf.keras.Model(inputs, outputs)\r\n",
        "  model.compile(\r\n",
        "      optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\r\n",
        "  )\r\n",
        "  model.summary()\r\n",
        "  return model\r\n",
        "\r\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozaX3cW0D55q"
      },
      "source": [
        "The main part of the model is imported as an already on the imagenet dataset pretrained model.\r\n",
        "\r\n",
        "The base model is concatenated with an input layer to define the shape the images are loaded in and preprocessed for the model.  \r\n",
        "\r\n",
        "In order to now realize the kfold validation the previously generated list of kfolds has to be iterated over. \r\n",
        "\r\n",
        "The executable code below defines both the models as described.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhd7f-4-9Ogf"
      },
      "source": [
        "def get_model_custom():\r\n",
        "    model = models.Sequential()\r\n",
        "    model.add(layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(IMG_H,IMG_W,IMG_C)))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Conv2D(64, (3,3), activation=\"relu\"))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Conv2D(128, (3,3), activation=\"relu\"))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Conv2D(128, (3,3), activation=\"relu\"))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Flatten())\r\n",
        "    model.add(layers.Dropout(0.5))\r\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\r\n",
        "    model.add(layers.Dense(NUM_LABELS, activation=\"sigmoid\"))\r\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\"acc\"])\r\n",
        "    return model\r\n",
        "\r\n",
        "def get_model_Effi():\r\n",
        "\r\n",
        "  # Adam \r\n",
        "  adam = tf.keras.optimizers.Adam(\r\n",
        "    learning_rate=0.0005,\r\n",
        "    name='Adam')\r\n",
        "  img_augmentation = Sequential(\r\n",
        "    [\r\n",
        "        preprocessing.RandomRotation(factor=0.15),\r\n",
        "        preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\r\n",
        "        preprocessing.RandomFlip(),\r\n",
        "        preprocessing.RandomContrast(factor=0.1),\r\n",
        "    ],\r\n",
        "    name=\"img_augmentation\",\r\n",
        "  )\r\n",
        "  NUM_CLASSES = 4\r\n",
        "  inputs = layers.Input(shape=(IMG_H, IMG_W, 3))\r\n",
        "  x = img_augmentation(inputs)\r\n",
        "  outputs = EfficientNetB1(include_top=True, weights=None, classes=NUM_CLASSES)(x)\r\n",
        "  model = tf.keras.Model(inputs, outputs)\r\n",
        "  model.compile(\r\n",
        "      optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\r\n",
        "  )\r\n",
        "\r\n",
        "  model.summary()\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNvMeJuX9PJe"
      },
      "source": [
        "```python\r\n",
        "for i,fold in enumerate(folds):\r\n",
        "```\r\n",
        "In this loop over each fold the train and test data set as well as the model are loaded for each iteration.\r\n",
        "### 4.2.2 Loading the labels and images \r\n",
        "For the loading process based on the list of folds a process loading the labels and images from a list of file paths had to be implemented.\r\n",
        "For this the following helper functions are used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smFZaVShAx1M"
      },
      "source": [
        "def get_file_list(dir):\r\n",
        "    #Liste mit kompletten Pfaden der Bilder\r\n",
        "    return [os.path.join(dir,bild) for bild in os.listdir(dir)]\r\n",
        "def get_labels(bilder):\r\n",
        "    matching_dict = ['_'+ os.path.basename(bild).split('_',1)[1].replace('.jpg','') for bild in bilder]\r\n",
        "    labels = np.array([dict_values.index(matching_str) for matching_str in matching_dict])\r\n",
        "    return to_categorical(labels,NUM_LABELS)\r\n",
        "def get_images(bilder):\r\n",
        "    num_images = len(bilder)\r\n",
        "    images = np.empty((num_images,IMG_H,IMG_W,IMG_C),dtype = np.float32)\r\n",
        "    for i,bild in enumerate(bilder):\r\n",
        "      image = load_img(bild, target_size = (IMG_H,IMG_W))\r\n",
        "      image = img_to_array(image)\r\n",
        "      image/=255\r\n",
        "      images[i,:,:,:] = image\r\n",
        "    return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMI3EhVcA-Yg"
      },
      "source": [
        "The function get_file_list() returns all filenames for a given directory.\r\n",
        "\r\n",
        "So by handing the function the test and train directory in each fold iteration a list of file paths for the train and test set is generated. On that base the labels and images for each file path can be loadad\r\n",
        "\r\n",
        "This is done in the following functions get_labels() and get_images(). Handed a list of file paths to jpg files the functions return a numpy array with either the pixels of the different images or the one hot encoded labels with a 1 standing for the correct label. The other positions are set to 0.\r\n",
        "The label is determined by extracting the values of the dictionary used in the labeling process isolating the label saved in the path string and assigning the index of the corresponding entry of the dictionary as label.\r\n",
        "The resulting numpy array after the one hot encoding process with four labels to classify between could look like this : [0, 0, 1, 0].\r\n",
        "\r\n",
        "If a list of four paths is given the resulting numpy arrays also contains data for four files either one hot encoded labels or the pixel data of the jpg file.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE11lut9QRtS"
      },
      "source": [
        "### 4.2.3 The Training Loop\r\n",
        "```python\r\n",
        "get_model = get_model_Eff\r\n",
        "BATCH_SIZE = 8\r\n",
        "EPOCHS = 35\r\n",
        "RES_DIR = 'results'\r\n",
        "if not os.path.exists(RES_DIR):\r\n",
        "    os.mkdir(RES_DIR)\r\n",
        "\r\n",
        "histories = []\r\n",
        "len_evals = []\r\n",
        "\r\n",
        "for i,fold in enumerate(folds):\r\n",
        "    tf.keras.backend.clear_session()  \r\n",
        "    test =  get_file_list(os.path.join(K_DIR,fold,'test'))\r\n",
        "    train = get_file_list(os.path.join(K_DIR,fold,'train'))\r\n",
        "    y_test = get_labels(test)\r\n",
        "    x_test = get_images(test)\r\n",
        "    model = get_model()\r\n",
        "    sequence = Generator(train, BATCH_SIZE) \r\n",
        "    history = model.fit(sequence, epochs = EPOCHS, validation_data = (x_test,y_test))\r\n",
        "    model.save(f'{RES_DIR}/k{i+1}.h5')\r\n",
        "    len_evals.append(len(test))\r\n",
        "    histories.append(history.history)\r\n",
        "```\r\n",
        "The code block shown above is responsible for the training itself and by starting it the training process for all generated folds is set into motion. Before the actual iteration over the folds and therefore the separate training cycles the network structure and some hyperparameters are defined.\r\n",
        "The get_model variable stores the model generation method and by that the model that shall be used for the training cycle. \r\n",
        "The batch_size defines how many pictures the network processes simultaneously for weight adjustments. The hyperparameter epochs defines how often the training set shall be iterated through.\r\n",
        "\r\n",
        "The code below shows the first configuration of the training loop in which the training set as well as the test set are directly loaded into RAM. \r\n",
        "\r\n",
        "```python\r\n",
        "\r\n",
        "for i,fold in enumerate(folds):\r\n",
        "    tf.keras.backend.clear_session()  \r\n",
        "    test =  get_file_list(os.path.join(K_DIR,fold,'test'))\r\n",
        "    train = get_file_list(os.path.join(K_DIR,fold,'train'))\r\n",
        "    y_train = get_labels(test)\r\n",
        "    x_train = get_images(train)\r\n",
        "    y_test = get_labels(test)\r\n",
        "    x_test = get_images(test)\r\n",
        "    model = get_model()\r\n",
        "    history = model.fit(X = x_train,y = y_train , epochs = EPOCHS, validation_data = (x_test,y_test))\r\n",
        "    model.save(f'{RES_DIR}/k{i+1}.h5')\r\n",
        "    len_evals.append(len(test))\r\n",
        "    histories.append(history.history)\r\n",
        "```\r\n",
        "Because on all systems the code ran on a RAM exceeded error was thrown a RAM friendly solution had to be found.\r\n",
        "\r\n",
        "### 4.2.4 Resolving RAM issues with the Sequence interface\r\n",
        "The idea was to not load the complete training set into memory directly. Instead the Sequence interface imported from the tf.keras.utils package  is used. \r\n",
        "The class feeds data to the model during the training process in a self designed way.\r\n",
        "The function __ len __ () defines the number of batches available with the given batch size and the length of the training data for each epoch and is mandatory for a functioning Sequence class.\r\n",
        "The __ get_item __() function shown below is called each time a new batch shall be loaded. It automatically is handed the current batch index. The matching images are determined by calculating the first and last image of the batch in the given list of file paths based on the current batch index. \r\n",
        "The loading process behind this is already explained in chapter 3.3.2.\r\n",
        "As seen below for each batch the described methods for obtaining the training images and labels are called by handing them the previously determined batch matching file paths as a list. This way the batches are loaded step by step from the hard drive which resolves the previously encountered RAM issue.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MVCtzfrW7QD"
      },
      "source": [
        "class Generator(tf.keras.utils.Sequence):\r\n",
        "    def __init__(self,img_files,batch_size):\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.batches = int(len(img_files)/batch_size)\r\n",
        "        self.img_files = img_files\r\n",
        "        self.imgs = len(img_files)\r\n",
        "        self.rest = self.imgs-self.batches * self.batch_size\r\n",
        "    def __len__(self):\r\n",
        "        if len(self.img_files)%self.batch_size == 0 : return self.batches\r\n",
        "        elif len(self.img_files)%self.batch_size>0 :return self.batches+1 \r\n",
        "        \r\n",
        "                   \r\n",
        "    def __getitem__(self,idx):\r\n",
        "        \r\n",
        "        start_index = idx * self.batch_size\r\n",
        "        if idx < self.batches:\r\n",
        "            last_index = (idx+1) *self.batch_size   \r\n",
        "        else :\r\n",
        "            if self.rest >0 : last_index = self.imgs\r\n",
        "            else: last_index = (idx+1)*self.batch_size\r\n",
        "        if start_index == last_index : last_index = start_index +1 \r\n",
        "        current = self.img_files[start_index: last_index]\r\n",
        "        \r\n",
        "        X = get_images(current)\r\n",
        "        y = get_labels(current)\r\n",
        "        print(' ',self.imgs,'start',start_index,'end',last_index)\r\n",
        "        return X,y \r\n",
        "    \r\n",
        "    def on_epoch_end(self):\r\n",
        "        random.shuffle(self.img_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3vhsfoUqWoA"
      },
      "source": [
        "The sequence class is handed instead of the arguments X and y to the fit call of the defined model. A functioning and executable training loop with the implemented data generator for the model looks as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCHRHHuAq1ba"
      },
      "source": [
        "get_model = get_model_Effi#get_model_custom\r\n",
        "BATCH_SIZE = 8\r\n",
        "EPOCHS = 40\r\n",
        "RES_DIR = 'results'\r\n",
        "if not os.path.exists(RES_DIR):\r\n",
        "    os.mkdir(RES_DIR)\r\n",
        "\r\n",
        "histories = []\r\n",
        "len_evals = []\r\n",
        "\r\n",
        "for i,fold in enumerate(folds):\r\n",
        "    tf.keras.backend.clear_session()  \r\n",
        "    test =  get_file_list(os.path.join(K_DIR,fold,'test'))\r\n",
        "    train = get_file_list(os.path.join(K_DIR,fold,'train'))\r\n",
        "    y_test = get_labels(test)\r\n",
        "    x_test = get_images(test)\r\n",
        "    model = get_model()\r\n",
        "    sequence = Generator(train, BATCH_SIZE) \r\n",
        "    history = model.fit(sequence, epochs = EPOCHS, validation_data = (x_test,y_test))\r\n",
        "    model.save(f'{RES_DIR}/k{i+1}.h5')\r\n",
        "    len_evals.append(len(test))\r\n",
        "    histories.append(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MUDY_qnrrkl"
      },
      "source": [
        "This starts the training process.\r\n",
        "In this loop the results of the training process are saved as a so called history object containing all metrics on the test and train after each epoch  aswell as the length of the observed test set for later evaluations. Also the trained model is saved to the previously created 'results' folder for the later export to the Raspberry Pi.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zu7k05wu7E7"
      },
      "source": [
        "### 4.2.4 Model evaluation\r\n",
        "\r\n",
        "For evaluation the graphics library matplotlib and the table processing library pandas are used.\r\n",
        "\r\n",
        "The during the training process stored metrics in the history object are used together with the test set lengths to determine the model performance.\r\n",
        "\r\n",
        "As a first step history objects are transformed to pandas dataframes for easier data handling.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hh8cOepyFsN"
      },
      "source": [
        "import pandas as pd\r\n",
        "historys = [pd.DataFrame(history) for history in histories]\r\n",
        "\r\n",
        "def average_metrics(historys,k,weights):\r\n",
        "    summe = sum(weights)\r\n",
        "    average = pd.DataFrame(data = np.zeros(shape = historys[0].shape,dtype = np.float32), columns = historys[0].columns)\r\n",
        "    for i, history in enumerate(historys):\r\n",
        "       average = average.add(history.apply(lambda x: x*weights[i]/summe))\r\n",
        "    return  average\r\n",
        "\r\n",
        "average = average_metrics(historys,K_FOLDS,len_evals)\r\n",
        "print(average)\r\n",
        "fig,ax = plt.subplots(1)\r\n",
        "ax.plot(average['val_accuracy'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-3YBqfSvNVz"
      },
      "source": [
        "This is done in the first two lines of the above executable code block. First the pandas package is imported to afterwards convert the history objects to pandas dataframes containing all metrics for the k training processes in an addressable table format.\r\n",
        "\r\n",
        "The dataframes are handed to the average_metrics() function which calculates the mean over the k folds for every metric using the size of each test set stored in the list len_evals list as weights.\r\n",
        "\r\n",
        "To achieve that each cell for each resulting dataframe is multiplied by its test set length divided by the length of the complete dataset. The result of this calculation is added to a dataframe opened before iterating over the k dataframes. This dataframe containing the average metrics over all k folds can now be returned.\r\n",
        "\r\n",
        "For visualization and evaluation purposes the average validation accuracy can be plotted using matplotlib. \r\n",
        "The library offers an interface for pandas dataframes so the generated column for the mean validation accuracy can easily be plotted by specifying the matching column. \r\n",
        "\r\n",
        "A plot representing a good training result and model performance for the tested EfficientNetB1 configuration is shown below.\r\n",
        "\r\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/carsten/Graph.jpg?raw=1\" width=400>\r\n",
        "\r\n",
        "After about 20 training iterations all models trained in the validation process show a better than 95% accuracy in classifying the images on the test dataset served which is a sign of well trained neural net work. The saved models for this run are eligible for further usage on the raspberry pi.\r\n",
        "\r\n",
        "### 4.2.5 Tflite conversion\r\n",
        "In order to be able to use the model on the raspberry pi, the model has to be converted into a tflite model because the raspberry pi runtime only offers limited resources. This also marks the last part of the preprocessing pipeline resulting in a ready for export machine learning model.\r\n",
        "\r\n",
        "To achieve this the previously in the results folder saved .h5 files containing the tensorflow models with the trained weights are used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glZ-0VanHIwS"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "for i in range(1,11):\r\n",
        "  path = f'results/k{i}.h5'\r\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(tf.keras.models.load_model(path))\r\n",
        "  tflite_model = converter.convert()\r\n",
        "  # Save the model.\r\n",
        "  with open(f'k{i}.tflite', 'wb') as f:\r\n",
        "    f.write(tflite_model)\r\n",
        "    print(f'tflite Model written to: k{i}.tflite')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHst6PHUvjcN"
      },
      "source": [
        "The executable code block above iterates over each of the k trained models during the validation process and as a first step loads them with the previously from the tensorflow package imported load_model() function handing it the models file location.\r\n",
        "\r\n",
        "The models are afterwards converted by the TFLiteConverter function from_keras_model(). The models are now ready for usage on the raspberry pi platform and can be saved to the working directory. This is done in the 'with open' block used for input/output operations including files.\r\n",
        "\r\n",
        "The models are now ready for export. This export can be executed by either simply downloading the files and loading them into the raspberry pi. An alternative option is uploading them to our self hosted fileserver by using the FTP protocol for file transfer. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCsnJlZh3Z95"
      },
      "source": [
        "# 5. Conclusions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyVHTYVZJm91"
      },
      "source": [
        "## 5.1. Testing the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxMRP2JdaLr7"
      },
      "source": [
        "### 5.1.1. The Testing Setup\n",
        "\n",
        "In order to get the most out of the machine learning process, several models were trained with different efficientNet models. Also, the performance of the first established custom-models will be shown. \n",
        "\n",
        "For testing purposes new plants were bought and some of the older dryer and grown plants will also be used. The setup follows the described one in Chapter 2:\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/setup.png?raw=1\" width=900/>\n",
        "\n",
        "**Fig ** - Testing Setup\n",
        "\n",
        "The following model-configurations will be tested against the test-plants:\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <th>Efficent Net</th>\n",
        "    <th>Image Modification</th>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>\n",
        "       B1, B3\n",
        "      </td>\n",
        "      <td>\n",
        "       Clean\n",
        "      </td>\n",
        "    </tr>\n",
        "     <tr>\n",
        "      <td>\n",
        "       B1, B3\n",
        "      </td>\n",
        "      <td>\n",
        "       Masked (Green)\n",
        "      </td>\n",
        "    </tr>\n",
        "     <tr>\n",
        "      <td>\n",
        "       B1, B3\n",
        "      </td>\n",
        "      <td>\n",
        "       Masked + Edge Detection (Canny)\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Z8_af_HeB4"
      },
      "source": [
        "In detail the modifications are below:\n",
        "\n",
        " - **Clean**: The captured image will be processed as it is, also the model was trained with the corresponding trainingsset.\n",
        "\n",
        " - **Masked**: Same as above just with the green HSV-mask applied\n",
        "\n",
        " - **Masked + Edge Detection**: Additionally the masked images were passed into the canny functions to detect the edges. \n",
        "\n",
        "The tables are divided into 5 colums:\n",
        "\n",
        " 1. Model - the current model in the iteration process\n",
        " 2. Label - the current label predicted against (the right label)\n",
        " 3. Hits - Number of right guesses\n",
        " 4. Misses - Number of wrong guesses\n",
        " 5. Average Certainty - the average certainty over all hits and misses\n",
        "\n",
        "The test plants were not in the best condition and totally different from the ones the models were trained with:\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/images/14_bd.jpg?raw=1\" width=200>\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/images/14_bf.jpg?raw=1\" width=200>\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/images/14_pd.jpg?raw=1\" width=200>\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/images/14_pf.jpg?raw=1\" width=200>\n",
        "\n",
        "**Fig ** - Test Plants\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yt6vB54J3NP"
      },
      "source": [
        "### 5.1.2. The Testing Results\n",
        "\n",
        "#### Model: B1 - Image Modification (From left to right): Clean, Masked, Masked + Edge Detection\n",
        "\n",
        "<table>\n",
        "    <thead>\n",
        "        <th>B1 Clean</th>\n",
        "        <th>B1 Mask</th>\n",
        "        <th>B1 Canny</th>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <td>\n",
        "            <table>\n",
        "                <tr>\n",
        "                    <th>Model</th>\n",
        "                    <th>Label</th>\n",
        "                    <th>Hits</th>\n",
        "                    <th>Misses</th>\n",
        "                    <th>Average Certainty</th>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>12</td>\n",
        "                    <td>4</td>\n",
        "                    <td>0.8977468200027943</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>4</td>\n",
        "                    <td>11</td>\n",
        "                    <td>0.8463258783022563</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9277591069539388</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.9243790762765067</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k10.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>1</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.903262909501791</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k10.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>13</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.9247966488202413</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k10.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>1</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0.7731154402097066</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k10.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>10</td>\n",
        "                    <td>4</td>\n",
        "                    <td>0.8832948846476418</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>9</td>\n",
        "                    <td>7</td>\n",
        "                    <td>0.9263387471437454</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>13</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.8935396591822307</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9443325122197469</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>10</td>\n",
        "                    <td>4</td>\n",
        "                    <td>0.9157448134252003</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>9</td>\n",
        "                    <td>7</td>\n",
        "                    <td>0.7804823368787766</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>7</td>\n",
        "                    <td>8</td>\n",
        "                    <td>0.7793381949265797</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.898945939540863</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>8</td>\n",
        "                    <td>6</td>\n",
        "                    <td>0.958035443510328</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>16</td>\n",
        "                    <td>0.9677308201789856</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>1</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0.9292452772458394</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9227205872535705</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0</td>\n",
        "                    <td>0.9850476000990186</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>16</td>\n",
        "                    <td>0.9998637773096561</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>14</td>\n",
        "                    <td>1</td>\n",
        "                    <td>0.9699569384256999</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9995128393173218</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>5</td>\n",
        "                    <td>9</td>\n",
        "                    <td>0.9184957529817309</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>7</td>\n",
        "                    <td>9</td>\n",
        "                    <td>0.9173370227217674</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0</td>\n",
        "                    <td>0.9344570914904277</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9727869709332784</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>3</td>\n",
        "                    <td>11</td>\n",
        "                    <td>0.8629610730069024</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>1</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9753190986812115</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0</td>\n",
        "                    <td>0.9803851207097372</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9901793479919434</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>1</td>\n",
        "                    <td>13</td>\n",
        "                    <td>0.8219583438975471</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>16</td>\n",
        "                    <td>0.9073465876281261</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>3</td>\n",
        "                    <td>0.8897831916809082</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>2</td>\n",
        "                    <td>13</td>\n",
        "                    <td>0.9154245297114054</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>4</td>\n",
        "                    <td>10</td>\n",
        "                    <td>0.7912498648677554</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>9</td>\n",
        "                    <td>7</td>\n",
        "                    <td>0.7337145935744047</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9658226569493612</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>1</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0.7670344710350037</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>13</td>\n",
        "                    <td>1</td>\n",
        "                    <td>0.8980935854571206</td>\n",
        "                </tr>\n",
        "            </table>\n",
        "        </td>\n",
        "        <td>\n",
        "            <table>\n",
        "                <tr>\n",
        "                    <th>Model</th>\n",
        "                    <th>Label</th>\n",
        "                    <th>Hits</th>\n",
        "                    <th>Misses</th>\n",
        "                    <th>Average Certainty</th>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>10</td>\n",
        "                    <td>6</td>\n",
        "                    <td>0.9287579730153084</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>4</td>\n",
        "                    <td>11</td>\n",
        "                    <td>0.9385746121406555</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>2</td>\n",
        "                    <td>13</td>\n",
        "                    <td>0.8847673535346985</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>8</td>\n",
        "                    <td>6</td>\n",
        "                    <td>0.9231294052941459</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k10.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>16</td>\n",
        "                    <td>0.9767634458839893</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k10.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>3</td>\n",
        "                    <td>0.9659349799156189</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k10.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9659468730290731</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k10.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>8</td>\n",
        "                    <td>6</td>\n",
        "                    <td>0.9641737384455544</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>5</td>\n",
        "                    <td>11</td>\n",
        "                    <td>0.6916189529001713</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>14</td>\n",
        "                    <td>1</td>\n",
        "                    <td>0.9665435910224914</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>1</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0.7516469935576121</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>11</td>\n",
        "                    <td>3</td>\n",
        "                    <td>0.8956677658217294</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>15</td>\n",
        "                    <td>1</td>\n",
        "                    <td>0.9956440851092339</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>14</td>\n",
        "                    <td>1</td>\n",
        "                    <td>0.9711408098538716</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>2</td>\n",
        "                    <td>13</td>\n",
        "                    <td>0.9298599382241567</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>0</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0.8459889271429607</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>14</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.9256935380399227</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>14</td>\n",
        "                    <td>1</td>\n",
        "                    <td>0.9861153483390808</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>12</td>\n",
        "                    <td>3</td>\n",
        "                    <td>0.812870979309082</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>0</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0.8697599044867924</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>1</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.8644116539508104</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>3</td>\n",
        "                    <td>0.9076174259185791</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.8755496303240459</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>10</td>\n",
        "                    <td>4</td>\n",
        "                    <td>0.9457120469638279</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>16</td>\n",
        "                    <td>0.8770254664123058</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>3</td>\n",
        "                    <td>0.906526792049408</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>10</td>\n",
        "                    <td>5</td>\n",
        "                    <td>0.8272612671057383</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.8738083243370056</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>16</td>\n",
        "                    <td>0.9929125681519508</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0</td>\n",
        "                    <td>0.997539508342743</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9899481932322184</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>2</td>\n",
        "                    <td>12</td>\n",
        "                    <td>0.9760545236723763</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>14</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.9143422413617373</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>13</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.9730483293533325</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.8675963203112285</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>5</td>\n",
        "                    <td>9</td>\n",
        "                    <td>0.9437119322163718</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>0</td>\n",
        "                    <td>16</td>\n",
        "                    <td>0.8550811558961868</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>13</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.9545910000801087</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>1</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0.8588677962621053</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.9297475346497127</td>\n",
        "                </tr>\n",
        "            </table>\n",
        "        </td>\n",
        "        <td>\n",
        "            <table>\n",
        "                <tr>\n",
        "                    <th>Model</th>\n",
        "                    <th>Label</th>\n",
        "                    <th>Hits</th>\n",
        "                    <th>Misses</th>\n",
        "                    <th>Average Certainty</th>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>5</td>\n",
        "                    <td>11</td>\n",
        "                    <td>0.8015683889389038</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>6</td>\n",
        "                    <td>9</td>\n",
        "                    <td>0.8389766295750936</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>10</td>\n",
        "                    <td>5</td>\n",
        "                    <td>0.962977393468221</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k1.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0</td>\n",
        "                    <td>0.9930234934602465</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>10</td>\n",
        "                    <td>6</td>\n",
        "                    <td>0.8643116652965546</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9752880215644837</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>10</td>\n",
        "                    <td>5</td>\n",
        "                    <td>0.9625486532847086</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k2.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.7843382443700518</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>9</td>\n",
        "                    <td>7</td>\n",
        "                    <td>0.80987898260355</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9818496505419413</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>9</td>\n",
        "                    <td>6</td>\n",
        "                    <td>0.9242906967798868</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k3.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.8778395525046757</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>15</td>\n",
        "                    <td>1</td>\n",
        "                    <td>0.9256905037909746</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9950221697489421</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>10</td>\n",
        "                    <td>5</td>\n",
        "                    <td>0.929548994700114</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k4.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>4</td>\n",
        "                    <td>10</td>\n",
        "                    <td>0.7827631235122681</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>7</td>\n",
        "                    <td>9</td>\n",
        "                    <td>0.8279833383858204</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>1</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0.9826628605524699</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>10</td>\n",
        "                    <td>5</td>\n",
        "                    <td>0.8802563130855561</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k5.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.9346793975148883</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>9</td>\n",
        "                    <td>7</td>\n",
        "                    <td>0.7476876601576805</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>1</td>\n",
        "                    <td>14</td>\n",
        "                    <td>0.8813283801078796</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>8</td>\n",
        "                    <td>7</td>\n",
        "                    <td>0.887295126914978</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k6.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>10</td>\n",
        "                    <td>4</td>\n",
        "                    <td>0.8716932024274554</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>16</td>\n",
        "                    <td>0</td>\n",
        "                    <td>0.9999332278966904</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9998027761777242</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>5</td>\n",
        "                    <td>10</td>\n",
        "                    <td>0.8253725488980611</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k7.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>2</td>\n",
        "                    <td>12</td>\n",
        "                    <td>0.8614381871053151</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>9</td>\n",
        "                    <td>7</td>\n",
        "                    <td>0.8159420751035213</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9915241400400797</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>7</td>\n",
        "                    <td>8</td>\n",
        "                    <td>0.8536506851514181</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k8.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>5</td>\n",
        "                    <td>9</td>\n",
        "                    <td>0.7515865032161985</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>basil_fresh</td>\n",
        "                    <td>12</td>\n",
        "                    <td>4</td>\n",
        "                    <td>0.8808272890746593</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>basil_dry</td>\n",
        "                    <td>0</td>\n",
        "                    <td>15</td>\n",
        "                    <td>0.9479344844818115</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>parsley_fresh</td>\n",
        "                    <td>4</td>\n",
        "                    <td>11</td>\n",
        "                    <td>0.7753950079282125</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>k9.h5</td>\n",
        "                    <td>parsley_dry</td>\n",
        "                    <td>12</td>\n",
        "                    <td>2</td>\n",
        "                    <td>0.9339552457843509</td>\n",
        "                </tr>\n",
        "            </table>\n",
        "        </td>\n",
        "    </tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}