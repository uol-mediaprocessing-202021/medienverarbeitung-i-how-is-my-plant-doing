{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLANTOMATIC_coursework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/carsten/plantomatic_coursework_mediaprocessing_group_wip_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp_M7EMFQTCF"
      },
      "source": [
        "**P L A N T O M A T I C** - COURSEWORK\n",
        "\n",
        "---\n",
        "\n",
        "*Kian Lütke, Carsten Montag, Johannes Maximilian Stürenburg*\n",
        "\n",
        "This document outlines the documentation and progress journal of group I of the class _Media-Processing_. This work is divided in several chapters. Each section will be presented with executable code examples which will be - if run in the correct order - in the end illustrate the whole project together. \n",
        "\n",
        "The same code will be appended in the appendix to be run alone; not tied to this course work.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFJa8RtR7Q8b"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "## 1.1. Motivation\n",
        "\n",
        "This coursework is part of a multi-project class called _Mediaprocessing_ taught by Prof. Dr. techn. Susanne Boll-Westermann and Dr.-Ing. Larbi Abdenebaoui. During the introduction weeks several Projects were introduced to pick from. As one of the group members does grow herbs at home the obvious choice here is the project _How Is My Plant Doing_. To briefly explain the goal subject: An image of a plant should be analyzed to tell whether the plant needs watering or not.  \n",
        "\n",
        "## 1.2. Case/Goal\n",
        "\n",
        "The main goal of the project is to distinguish between two different plants and classify them to get an idea about their current state. \n",
        "\n",
        "A case was developed to fund the classwork’s structure upon:\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/1-concept.png?raw=1\" height=\"300px\" alt=\"Concept\" id=\"fig-concept\">\n",
        "\n",
        "**Fig 1** - Rough Concept\n",
        "\n",
        "The idea outlines that some sort of robot will take care of a kitchen-garden. The robot should move from plant to plant taking pictures and processing the images. Afterwards the images will be analyzed and classified. The robot decides whether the plant needs to be watered or not. In the letter case a message will be send to the owner’s mobile device to inform about the plants state. The message could contain the image of the plant itself as well. As there is no access to a real robot which actually is able to move. This project will make use of a stationary Raspberry Pi computer with a camera attached to it. The Raspberry Pi will then take a set of pictures every hour and evaluate the plant-state directly.    \n",
        "\n",
        "## 1.3. Methodology\n",
        "This work is divided in two main sections: The feature analysis part and the machine learning part. The first part will cover how images were gathered and how these images will be processed in order to prepare them for the machine learning training. In the second part the actual learning of the machine learning algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKoiNLFw7zYC"
      },
      "source": [
        "# 2. Feature Analysis\n",
        "\n",
        "## 2.1. Data Gathering\n",
        "\n",
        "This section describes the process of acquiring images of the plants. The first approach was to use a mirrorless system-camera to take thousands of pictures.\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/2-dslm.png?raw=1\" height=\"300px\" alt=\"Concept\" id=\"fig-dslm\">\n",
        "\n",
        "**Fig 2** - Picture taken with a DSLM Full Frame Camera [Sony a7 – 50mm/f5.6]\n",
        "\n",
        "As seen in Figure 2 the results are astonishing. The downside comes with the amount of time needed to take all the needed images. Taking approx. 1200 images took around 3 hours. So, the idea came up to take short videos of the plant and extract single frames to process them as images. For example, a short clip of 30 seconds could generate 1800 images if every fame is used.\n",
        "\n",
        "60 FPS Footage: 30 (seconds) * 60 (fps) = 1800\n",
        "\n",
        "The drawback here is the blurriness of extracted frames. A solution will be discussed in the next section.\n",
        "\n",
        "**Choosing the right plants**\n",
        "\n",
        "At the beginning of the projects two different plants were chosen: Basil and German mint. During this project we decided – because of the difficulties to distinguish between this two plants  - to switch from mint to chives. We decided to create a diary and observe the plants in different states. More on that in the next chapter.\n",
        "\n",
        "## 2.2. The Plant-Diary\n",
        "\n",
        "> Date | State of Chive | Sate of Basil | Video Chive | Video Basil | Notes | Time passed (h) since recovery \n",
        "> --- | --- | --- | --- | --- | --- | ---\n",
        "> 09.12.20 12:30 | New | Needs watering | ------------------------------------------------------ | ------------------------------------------------------ | Last watering | n/a\n",
        "> 09.12.20 17:00 | No change visible | Fully recovered | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch1.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum1.gif?raw=1\" height=\"100\"> |  | 0\n",
        "> 10.12.20 12:10 | No change visible | No change visible | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch2.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum2.gif?raw=1\" height=\"100\"> |  | 19\n",
        "> 10.12.20 19:50 | Visible hanging of the leaves | Pot is very light, will need water soon | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch3.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum3.gif?raw=1\" height=\"100\"> |  | 27\n",
        "> 11.12.20 11:50 | Even more hanging | Leaves also start hanging; pot is very light; needs water | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch4.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum4.gif?raw=1\" height=\"100\"> |  | 43\n",
        "> 12.12.20 09:10 | Still green; growing fast | End of experiment; final state is reached | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch5.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum5.gif?raw=1\" height=\"100\"> | Basil gets watered | 64\n",
        "> 13.12.20 14:30 | Still hanging | Fully recovered again | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch6.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum6.gif?raw=1\" height=\"100\"> | Chive gets watered | 93\n",
        "> 14.12.20 12:30 | Still the same development | aborted | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch7.gif?raw=1\" height=\"100\">  | n/a |  | 116\n",
        "> 16.12.20 08:30 | Still the same development | aborted | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch8.gif?raw=1\" height=\"100\">  | n/a | Experiment aborted | 159\n",
        "\n",
        "## 2.3. Pre-Processing\n",
        "\n",
        "## 2.4. The Robot aka Raspberry Pi\n",
        "\n",
        "## 2.5. Telegram Bot\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eB8G4yjP5_q"
      },
      "source": [
        "# 3. Machine Learning\n",
        "\n",
        "## 3.1 Choosing the fitting machine learning approach \n",
        "- 3 verschiedene Ansätze\n",
        "-  warum nehmen wir Image recogn\n",
        "\n",
        "## 3.2 Definition and training of the model \n",
        "### 3.2.1 Labeling our data\n",
        "Because for our machine learning model we chose a Supervised Learning approach, the pictures generated and cut from our Plant-Diary Videos by our Preprocessing Pipeline had to be labeled in order to feed them to the model.\n",
        "The neural net after training shall be able to differenciate and classify between three states for each plant state recorded in our diary. For that we tried to fit the videos we produced into three categories to later attach the corresponding label to the filename. These labels are \"healthy\", \"in need of water\" and \"dried up\" for each plant. Also because over the duration of the project we changed the plants to classify a few times so we had to design the labeling process accordingly variable.  \n",
        "In order to do that we named the recorded videos matching its plants type and with a rising number matching the state of the plant. So a video named \"basil1\" would be a video of a fresh from store basil plant. The worse the state of the plant the higher the number following the plant name.\n",
        "With all of that in mind we began programming a class based label generator for our pipeline which we later refactored to produce our K Folds.\n",
        "The following code snippet shows the constructor of the class which will be explained in the textblock after."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B32ZYL3QcEMr"
      },
      "source": [
        "class KFoldGenerator():\r\n",
        "  def __init__(self,data_dir,plants):\r\n",
        "      self.RS = 69\r\n",
        "      random.seed = self.RS\r\n",
        "      self.plants = plants\r\n",
        "      self.data_dir = data_dir\r\n",
        "      self.nr_classes = len(plants)*3 \r\n",
        "      self.dir_list = [dir for dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,dir))]\r\n",
        "      if '.ipynb_checkpoints' in self.dir_list : \r\n",
        "        self.dir_list.remove('.ipynb_checkpoints')\r\n",
        "      self.plant_nrs = []\r\n",
        "      for plant in plants:\r\n",
        "          nrs = sorted([int(dir.replace(plant,'')) for dir in self.dir_list if plant in dir])\r\n",
        "          self.plant_nrs.append(nrs)\r\n",
        "      self.class_dict = {}\r\n",
        "      for i,plant in enumerate(self.plants):\r\n",
        "          nrs = self.plant_nrs[i]\r\n",
        "          self.class_dict.update({\r\n",
        "            f'{plant}{nrs[0]}' : f'_class_{plant}_frisch',\r\n",
        "            f'{plant}{nrs[1]}' : f'_class_{plant}_giessen',\r\n",
        "            f'{plant}{nrs[2]}' : f'_class_{plant}_vertrocknet',\r\n",
        "          })\r\n",
        "      \r\n",
        "      filepaths = self.__load_paths__()\r\n",
        "      self.filepaths = self.__set_class__(filepaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F7GGEtlipFF"
      },
      "source": [
        "The constructor takes an array of plant names to classify into its 3 states as strings together with the parent folder path in which the subfolders for the different classes are located. These sub-folders already contain the cropped images generated from the videos. \r\n",
        "Based on the number of plants given in the \"plants\" array the number of classes for the neural net to later classify between is set by multiplying the length of the array by three.-\r\n",
        "Following that a list comprehension determines the sub folder paths contained in the parent directory. The subdirectory names which are the base for each class are now stored in the class variable \"dir_list\".\r\n",
        "Based on that the numbers following the plant name in the directory name had to be ordered to allow for a dictionary to be created which is intended to be used to match a label to each given picture in those subdirectorys. This is done in the for-loop beginning in line 12. The loop iterates over the in the constructor passed string array to determine the corresponding directory numbers emerging from its video name for each label and plant ordered by plant state. The determined numbers for each plant are now used to create a dictionary with the plant name and directory number as key and the class as value to be used in the actual labelling process. This allows us to pick later diary entrys with higher video numbers for our neural net because with this approach the pipeline is able to treat [\"basil1.mp4\", \"basil2.mp4\", \"basil3.mp4\"] the same way it treats [\"basil2.mp4\", \"basil5.mp4\", \"basil8.mp4\"] without us being forced to manually change anything in the pipeline except the videos to generate cropped pictures from in our short bash script at the beginning of the pipeline.\r\n",
        "With all required preparations taken care of the actual labeling process is called in the last two lines of the __ init __() function.\r\n",
        "The function __ load_paths __() takes care of collecting the complete filepaths for each picture in the targeted subdirectorys into a single list and return it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSUXYux1zKYb"
      },
      "source": [
        "def __load_paths__(self):\r\n",
        "      all_paths = []\r\n",
        "      for dir in self.dir_list:\r\n",
        "         dir_path = os.path.join(self.data_dir,dir) \r\n",
        "         paths = [os.path.join(dir_path,file) for file in os.listdir(dir_path)]\r\n",
        "         all_paths.extend(paths)\r\n",
        "      return all_paths "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdfYf7IF1_Zf"
      },
      "source": [
        "Now the labels can be set which is done in the __ set_class __() function by applying the previously generated dictionary to each filepath in the \"filepaths\" list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvmhkB-Y2ZYK"
      },
      "source": [
        "def __set_class__(self,files):\r\n",
        "    classes = []\r\n",
        "    for file in files:\r\n",
        "      dir = os.path.dirname(file)\r\n",
        "      self.class_dict.get(os.path.basename(os.path.normpath(dir)))\r\n",
        "      tup = (file,self.class_dict.get(os.path.basename(os.path.normpath(dir))))\r\n",
        "      classes.append(tup) \r\n",
        "    random.shuffle(classes)\r\n",
        "    return classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfQ8S6Eh2gHI"
      },
      "source": [
        "To do that the function iterates over each image path, isolates the subdirectory path the image is stored in as a single string without special characters to match the dictionary keys. The resulting classes returned by the get() function of the dictionary str stored together with the complete filepath as a tuple which leaves us with a list of labels and imagepaths to be used for a single model training aswell as for our KFold generation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V_UFYwucFEQ"
      },
      "source": [
        "## 3.2.2 A KFold to enhance the expressiveness of our metrics\r\n",
        "\r\n",
        "## 3.2.3 The Sequence class used for the training process\r\n",
        "## 3.3 TFLite model conersation\r\n",
        " - h5 to tf lite how? Quelle \r\n",
        "## 3.4 Problems and solutions  \r\n",
        " - ressourcen (vorallem ram)\r\n",
        " - -> generator ansatz\r\n",
        " - overfitting? -> k-Fold"
      ]
    }
  ]
}