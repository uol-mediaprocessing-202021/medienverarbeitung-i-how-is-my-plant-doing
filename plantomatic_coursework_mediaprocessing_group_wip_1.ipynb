{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLANTOMATIC_coursework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/carsten/plantomatic_coursework_mediaprocessing_group_wip_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp_M7EMFQTCF"
      },
      "source": [
        "**P L A N T O M A T I C** - COURSEWORK\n",
        "\n",
        "---\n",
        "\n",
        "*Kian Lütke, Carsten Montag, Johannes Maximilian Stürenburg*\n",
        "\n",
        "This document outlines the documentation and progress journal of group I of the class _Media-Processing_. This work is divided in several chapters. Each section will be presented with executable code examples which will be - if run in the correct order - in the end illustrate the whole project together. \n",
        "\n",
        "The same code will be appended in the appendix to be run alone; not tied to this course work.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFJa8RtR7Q8b"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "## 1.1. Motivation\n",
        "\n",
        "This coursework is part of a multi-project class called _Mediaprocessing_ taught by Prof. Dr. techn. Susanne Boll-Westermann and Dr.-Ing. Larbi Abdenebaoui. During the introduction weeks several Projects were introduced to pick from. As one of the group members does grow herbs at home the obvious choice here is the project _How Is My Plant Doing_. To briefly explain the goal subject: An image of a plant should be analyzed to tell whether the plant needs watering or not.  \n",
        "\n",
        "## 1.2. Case/Goal\n",
        "\n",
        "The main goal of the project is to distinguish between two different plants and classify them to get an idea about their current state. \n",
        "\n",
        "A case was developed to fund the classwork’s structure upon:\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/1-concept.png?raw=1\" height=\"300px\" alt=\"Concept\" id=\"fig-concept\">\n",
        "\n",
        "**Fig 1** - Rough Concept\n",
        "\n",
        "The idea outlines that some sort of robot will take care of a kitchen-garden. The robot should move from plant to plant taking pictures and processing the images. Afterwards the images will be analyzed and classified. The robot decides whether the plant needs to be watered or not. In the letter case a message will be send to the owner’s mobile device to inform about the plants state. The message could contain the image of the plant itself as well. As there is no access to a real robot which actually is able to move. This project will make use of a stationary Raspberry Pi computer with a camera attached to it. The Raspberry Pi will then take a set of pictures every hour and evaluate the plant-state directly.    \n",
        "\n",
        "## 1.3. Methodology\n",
        "This work is divided in two main sections: The feature analysis part and the machine learning part. The first part will cover how images were gathered and how these images will be processed in order to prepare them for the machine learning training. In the second part the actual learning of the machine learning algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKoiNLFw7zYC"
      },
      "source": [
        "# 2. Feature Analysis\n",
        "\n",
        "## 2.1. Data Gathering\n",
        "\n",
        "This section describes the process of acquiring images of the plants. The first approach was to use a mirrorless system-camera to take thousands of pictures.\n",
        "\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/figures/2-dslm.png?raw=1\" height=\"300px\" alt=\"Concept\" id=\"fig-dslm\">\n",
        "\n",
        "**Fig 2** - Picture taken with a DSLM Full Frame Camera [Sony a7 – 50mm/f5.6]\n",
        "\n",
        "As seen in Figure 2 the results are astonishing. The downside comes with the amount of time needed to take all the needed images. Taking approx. 1200 images took around 3 hours. So, the idea came up to take short videos of the plant and extract single frames to process them as images. For example, a short clip of 30 seconds could generate 1800 images if every fame is used.\n",
        "\n",
        "60 FPS Footage: 30 (seconds) * 60 (fps) = 1800\n",
        "\n",
        "The drawback here is the blurriness of extracted frames. A solution will be discussed in the next section.\n",
        "\n",
        "**Choosing the right plants**\n",
        "\n",
        "At the beginning of the projects two different plants were chosen: Basil and German mint. During this project we decided – because of the difficulties to distinguish between this two plants  - to switch from mint to chives. We decided to create a diary and observe the plants in different states. More on that in the next chapter.\n",
        "\n",
        "## 2.2. The Plant-Diary\n",
        "\n",
        "> Date | State of Chive | Sate of Basil | Video Chive | Video Basil | Notes | Time passed (h) since recovery \n",
        "> --- | --- | --- | --- | --- | --- | ---\n",
        "> 09.12.20 12:30 | New | Needs watering | ------------------------------------------------------ | ------------------------------------------------------ | Last watering | n/a\n",
        "> 09.12.20 17:00 | No change visible | Fully recovered | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch1.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum1.gif?raw=1\" height=\"100\"> |  | 0\n",
        "> 10.12.20 12:10 | No change visible | No change visible | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch2.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum2.gif?raw=1\" height=\"100\"> |  | 19\n",
        "> 10.12.20 19:50 | Visible hanging of the leaves | Pot is very light, will need water soon | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch3.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum3.gif?raw=1\" height=\"100\"> |  | 27\n",
        "> 11.12.20 11:50 | Even more hanging | Leaves also start hanging; pot is very light; needs water | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch4.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum4.gif?raw=1\" height=\"100\"> |  | 43\n",
        "> 12.12.20 09:10 | Still green; growing fast | End of experiment; final state is reached | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch5.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum5.gif?raw=1\" height=\"100\"> | Basil gets watered | 64\n",
        "> 13.12.20 14:30 | Still hanging | Fully recovered again | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch6.gif?raw=1\" height=\"100\">  | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/basilikum6.gif?raw=1\" height=\"100\"> | Chive gets watered | 93\n",
        "> 14.12.20 12:30 | Still the same development | aborted | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch7.gif?raw=1\" height=\"100\">  | n/a |  | 116\n",
        "> 16.12.20 08:30 | Still the same development | aborted | <img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/master/gif/schnittlauch8.gif?raw=1\" height=\"100\">  | n/a | Experiment aborted | 159\n",
        "\n",
        "## 2.3. Pre-Processing\n",
        "\n",
        "## 2.4. The Robot aka Raspberry Pi\n",
        "\n",
        "## 2.5. Telegram Bot\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eB8G4yjP5_q"
      },
      "source": [
        "# 3. Machine Learning\n",
        "\n",
        "## 3.1 Choosing the fitting machine learning approach \n",
        "- 3 verschiedene Ansätze\n",
        "-  warum nehmen wir Image recogn\n",
        "\n",
        "## 3.2 Preparations for model training \n",
        "### 3.2.1 Labeling the data\n",
        "Because for the machine learning model a Supervised Learning approach was chosen, the pictures generated and cut from the Plant-Diary Videos by the Preprocessing Pipeline had to be labeled in order to feed them to the model.\n",
        "The neural net after training shall be able to differenciate and classify denpendant on model performance between two to three states for each plant recorded in our diary. For that the produced videos were fit into three categories to later attach the corresponding label to the filename. These labels are \"healthy\", \"in need of water\" and \"dried up\" for each plant. Because over the duration of the project the plants to classify were changed a few times so the labeling process had to be designed accordingly variable.\n",
        "In order to do that the recorded videos were named matching the given plants species and with a rising number matching the state of the plant. So a video named \"basil1\" would be a video of a fresh from store basil plant. The worse the state of the plant the higher the number following the plant name.\n",
        "With all of that in mind in the machine learning programming process began with conceiving a class based label generator for the pipeline which later was refactored to produce the K Folds.\n",
        "The following code snippet shows the constructor of the class which will be explained in the textblock after."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B32ZYL3QcEMr"
      },
      "source": [
        "class KFoldGenerator():\r\n",
        "  def __init__(self,data_dir,plants):\r\n",
        "      self.RS = 69\r\n",
        "      random.seed = self.RS\r\n",
        "      self.plants = plants\r\n",
        "      self.data_dir = data_dir\r\n",
        "      self.nr_classes = len(plants)*3 \r\n",
        "      self.dir_list = [dir for dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,dir))]\r\n",
        "      if '.ipynb_checkpoints' in self.dir_list : \r\n",
        "        self.dir_list.remove('.ipynb_checkpoints')\r\n",
        "      self.plant_nrs = []\r\n",
        "      for plant in plants:\r\n",
        "          nrs = sorted([int(dir.replace(plant,'')) for dir in self.dir_list if plant in dir])\r\n",
        "          self.plant_nrs.append(nrs)\r\n",
        "      self.class_dict = {}\r\n",
        "      for i,plant in enumerate(self.plants):\r\n",
        "          nrs = self.plant_nrs[i]\r\n",
        "          self.class_dict.update({\r\n",
        "            f'{plant}{nrs[0]}' : f'_class_{plant}_frisch',\r\n",
        "            f'{plant}{nrs[1]}' : f'_class_{plant}_giessen',\r\n",
        "            f'{plant}{nrs[2]}' : f'_class_{plant}_vertrocknet',\r\n",
        "          })\r\n",
        "      \r\n",
        "      filepaths = self.__load_paths__()\r\n",
        "      self.filepaths = self.__set_class__(filepaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F7GGEtlipFF"
      },
      "source": [
        "The constructor takes an array of plant names to classify into its two to three states as strings together with the parent folder path in which the subfolders for the different classes are located. These sub-folders already contain the cropped images generated from the videos. \r\n",
        "Based on the number of plants given in the \"plants\" array the number of classes for the neural net to later classify between is set by multiplying the length of the array by three.\r\n",
        "\r\n",
        "Following that a list comprehension determines the sub folder paths contained in the parent directory. The subdirectory names which are the base for each class are now stored in the class variable \"dir_list\".\r\n",
        "\r\n",
        "Based on that the numbers following the plant name in the directory name had to be ordered to allow for a dictionary to be created which is intended to be used to match a label to each given picture in those subdirectorys. This is done in the for-loop beginning in line 12.\r\n",
        "\r\n",
        "The loop iterates over the in the constructor passed string array to determine the corresponding directory numbers emerging from its video name for each label and plant ordered by plant state. The determined numbers for each plant are now used to create a dictionary with the plant name and directory number as key and the class as value to be used in the actual labelling process. This allows us to pick later diary entrys with higher video numbers for our neural net because with this approach the pipeline is able to treat [\"basil1.mp4\", \"basil2.mp4\", \"basil3.mp4\"] the same way it treats [\"basil2.mp4\", \"basil5.mp4\", \"basil8.mp4\"] without us being forced to manually change anything in the pipeline except the videos to generate cropped pictures from in our short bash script at the beginning of the pipeline.\r\n",
        "\r\n",
        "Also the number of states to classify between is easy to change by just configuring the classes dictionary and number of videos served to the pipeline.\r\n",
        "With all required preparations taken care of the actual labeling process is called in the last two lines of the __ init __() function.\r\n",
        "The function __ load_paths __() takes care of collecting the complete filepaths for each picture in the targeted subdirectorys into a single list and return it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSUXYux1zKYb"
      },
      "source": [
        "def __load_paths__(self):\r\n",
        "      all_paths = []\r\n",
        "      for dir in self.dir_list:\r\n",
        "         dir_path = os.path.join(self.data_dir,dir) \r\n",
        "         paths = [os.path.join(dir_path,file) for file in os.listdir(dir_path)]\r\n",
        "         all_paths.extend(paths)\r\n",
        "      return all_paths "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdfYf7IF1_Zf"
      },
      "source": [
        "Now the labels can be set which is done in the __ set_class __() function by applying the previously generated dictionary to each filepath in the \"filepaths\" list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvmhkB-Y2ZYK"
      },
      "source": [
        "def __set_class__(self,files):\r\n",
        "    classes = []\r\n",
        "    for file in files:\r\n",
        "      dir = os.path.dirname(file)\r\n",
        "      self.class_dict.get(os.path.basename(os.path.normpath(dir)))\r\n",
        "      tup = (file,self.class_dict.get(os.path.basename(os.path.normpath(dir))))\r\n",
        "      classes.append(tup) \r\n",
        "    random.shuffle(classes)\r\n",
        "    return classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfQ8S6Eh2gHI"
      },
      "source": [
        "To do that the function iterates over each image path, isolates the subdirectory path the image is stored in as a single string without special characters to match the dictionary keys. The resulting classes returned by the get() function of the dictionary str stored together with the complete filepath as a tuple which leaves us with a list of labels and imagepaths to be used for a single model training aswell as for the KFold generation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V_UFYwucFEQ"
      },
      "source": [
        "### 3.2.2 A KFold cross validation for performance measuring\r\n",
        "To determine the performance of a given model it has to be validated what means testing it on different test data sets. In Image Recognition in contrast to for example Time Series Analysis a simple so called kfold cross validation is the standard to validate the performance of a machine learning model.\r\n",
        "\r\n",
        "Performing a kfold cross validation means splitting the given dataset into k   splits with train and test data while each split contains 1/k of the original dataset as test set for validation. The remaining data is used for training. By that each picture of the complete data set is exactly once used for validation and with calculating the mean performance by multiplying each result with 1/k and adding them up afterwards a well validated perfomance measure in form of the validation accuracy is the result[1]. \r\n",
        "\r\n",
        "The following picture illustrates the concept very good. \r\n",
        "\r\n",
        "<img src=\"https://miro.medium.com/max/4984/1*kheTr2G_BIB6S4UnUhFp8g.png\" width=800>[2]\r\n",
        "\r\n",
        "To implement this the label generator class had to be extended with functions serving the purpose of generating the folds and copying the cropped pictures\r\n",
        "to newly created folders for each validation step.\r\n",
        "\r\n",
        "[1] Aurélien Géron, \"Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems\",O´Reilly Media,2019\r\n",
        "\r\n",
        "[2] https://miro.medium.com/max/4984/1*kheTr2G_BIB6S4UnUhFp8g.png\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfRee1AsIHw6"
      },
      "source": [
        "def generate_k_folds(self,k):\r\n",
        "    copy_list = self.filepaths[:]\r\n",
        "    test_size = floor(len(self.filepaths)/k)\r\n",
        "    rest = len(self.filepaths)%k\r\n",
        "    folds = []\r\n",
        "    for i in range(0,k):\r\n",
        "      if i == k-1:\r\n",
        "        test_size = test_size + rest\r\n",
        "      folds.append(self.__generate_fold__(copy_list,test_size))\r\n",
        "    dir_kfolds = 'K_FOLDS'  \r\n",
        "    os.mkdir('K_FOLDS')\r\n",
        "    for i,fold in enumerate(folds):\r\n",
        "      current_fold = f'{dir_kfolds}/Fold{i+1}'\r\n",
        "      train = fold[0]\r\n",
        "      test = fold[1]\r\n",
        "      print(test)  \r\n",
        "      os.mkdir(current_fold)\r\n",
        "      test_dir = f'{current_fold}/test'\r\n",
        "      train_dir = f'{current_fold}/train'\r\n",
        "      os.mkdir(test_dir)\r\n",
        "      os.mkdir(train_dir)  \r\n",
        "      self.__copy_list_to_dir__(test,test_dir)\r\n",
        "      self.__copy_list_to_dir__(train,train_dir)\r\n",
        "\r\n",
        "  def __generate_fold__(self,liste,size):\r\n",
        "    test = []\r\n",
        "    while len(test)<size:\r\n",
        "      test_item = random.choice(liste)\r\n",
        "      test.append(test_item)\r\n",
        "      liste.remove(test_item)\r\n",
        "    train = [file for file in self.filepaths if file not in test ]\r\n",
        "    return (train, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPJ6FlSPI2Df"
      },
      "source": [
        "The public method generate_k_folds() of the generator has to be called in order to generate the splits and copy them directly into the working directory. \r\n",
        "\r\n",
        "This is done by generating a copy of the in the constructor already labeled pictures, calculating the size for each split also taking the slightly bigger last split into consideration by calculating its overhead. \r\n",
        "\r\n",
        "In the for loop the function __ generate_fold __() is called k times each time taking 1/k*dataset_size random pictures from the list adding them to a new list. The original list entrys are removed from the copied list to make sure they are not taken into account in the next iteration. \r\n",
        "\r\n",
        "After generating the test set in the function the matching train set is generated by using the concept of list comprehension copying each filepath with its label not present in the test set.\r\n",
        "\r\n",
        "The function returns a tuple containg the train set on the first index and the test set on the second index. These tuples are added to the list folds which is iterated over again beginning in line 11 to copy the files according to the generated folds.\r\n",
        "\r\n",
        "For that purpose for each fold generated a folder is created with the name Fold_i containg two subfolders train and test. \r\n",
        "At the end of the public function the generated train and test lists are seperately handed to the __ copy_list_to_dir __() function together with the previously in the for loop created directory paths.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdVLSwCnMg6w"
      },
      "source": [
        "def __copy_list_to_dir__(self,list,dir):  \r\n",
        "    for item in list:\r\n",
        "      src = item[0]\r\n",
        "      label = item[1]\r\n",
        "      file_info = os.path.splitext(os.path.basename(src))\r\n",
        "      dst = f'{dir}/{file_info[0]}{label}{file_info[1]}'\r\n",
        "      copy2(src = src,dst = dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM6ZgvflPtUD"
      },
      "source": [
        "This function uses the copy2 function of the previously imported shutil package to copy a single file from a given source location handed over in the list of tuples to a destination location. \r\n",
        "\r\n",
        "This is generated for each picture in the list in line five by putting together the handed over arguments in the list of tuples. The new filepath is created by concatenating the original raw filename with the new folderpath and the generated label in a formatted string. This label can later be interpreted by the training algorithm.\r\n",
        "\r\n",
        "The result of executing the function is a directory containing all generated folds called \"K_FOLDS\" containing a train and a test folder for each training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6HG0t7tU1Q_"
      },
      "source": [
        "K_FOLDS = 10\r\n",
        "PLANTS = ['basilikum', 'petersilie']\r\n",
        "DIR ='plants'\r\n",
        "gen = KFoldGenerator(DIR,PLANTS)\r\n",
        "gen.generate_k_folds(K_FOLDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7S9N_oZU82z"
      },
      "source": [
        "So with the number of folds set to 10 the resulting folder produced by calling the function in line five of the above codeblock would look as follows : \r\n",
        "\r\n",
        "<img src=\"https://github.com/uol-mediaprocessing-202021/medienverarbeitung-i-how-is-my-plant-doing/blob/carsten/folds.jpg?raw=1\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZD3NA51IhFI"
      },
      "source": [
        "## 3.3 Training the models \r\n",
        "In order to train the models with the generated datasets the folders containing the images have to be placed in memory. \r\n",
        "This is done by calling the function listdir in the native python package \"os\".\r\n",
        "The function returns all folders and files for a given directory path as strings in a non ordered fashion. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjDiSIoGZY3Q"
      },
      "source": [
        "K_DIR = 'K_FOLDS'\r\n",
        "RES_DIR = 'results'\r\n",
        "NUM_LABELS = len(gen.class_dict)\r\n",
        "dict_values = list(gen.class_dict.values())\r\n",
        "folds  = os.listdir(K_DIR) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncvd6chMZZ03"
      },
      "source": [
        "So the list returned in line five with the previously generated folds shown in the picture in chapter 3.2.2 would contain the strings ['Fold1','Fold10','Fold2','Fold3','Fold4','Fold5','Fold6','Fold7','Fold8','Fold9']. This list has to be ordered by the number of the fold to train the models in the correct order. This is realized by ordering the list with the utility function natural_keys() using regular expressions shown below. \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSE9e2_ecghx"
      },
      "source": [
        "def atoi(text):\r\n",
        "    return int(text) if text.isdigit() else text\r\n",
        "def natural_keys(text):\r\n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\r\n",
        "sorted = folds.sort(key = natural_keys) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqut3M51ieFa"
      },
      "source": [
        "With the ordered subdirectories representing each fold representing the data in memory as a next step the Image Recognition models had to be defined.\r\n",
        "\r\n",
        "### 3.3.1 Defining the model\r\n",
        "For the neural network a self configured model consisting of some Conv2D layers always followed by a MaxPooling2D layer was tested first. \r\n",
        "\r\n",
        "The Output for each tested model is generated using a Dense layer as output containing an output node for each assigned label. In this case a Flatten layer is included between the basic structure and the Output nodes. \r\n",
        "\r\n",
        "If an image is shown to the network, while or after training, the model outputs a percentage value for each class. The node with the highest value represents the class the neural network predicted for this picture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp6OHk1Tvoj0"
      },
      "source": [
        "\r\n",
        "The network is trained by serving it the pixels as a numpy array together with the desired label also as a numpy array as input. The network trains by adjusting its weights each time a batch of pictures is served. This is done according to the assigned loss function which in this case is the categorical crossentropy. It measures the error between the nets predictions and the actual label for each batch of images shown. \r\n",
        "This value together with the loss function at the end of each epoch of the training process defines the amount each weight in the neural network has to be changed. By that a high loss value results in a greater change of weights than a low loss value does.\r\n",
        "To make sure the neural network doesnt just learn the images by heart the network is shown the test dataset which it hasnt been trained on each epoch. By calculating the accuracy for the test dataset the model performance is measured.\r\n",
        "\r\n",
        "For each of the different models tested a seperate function returning the compiled model was created. \r\n",
        "\r\n",
        "The defintion for the originally used basic model is shown in the codeblock below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wknx56U1xGDK"
      },
      "source": [
        "def get_model_custom():\r\n",
        "    model = models.Sequential()\r\n",
        "    model.add(layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(IMG_H,IMG_W,IMG_C)))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Conv2D(64, (3,3), activation=\"relu\"))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Conv2D(128, (3,3), activation=\"relu\"))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Conv2D(128, (3,3), activation=\"relu\"))\r\n",
        "    model.add(layers.MaxPooling2D((2,2)))\r\n",
        "    model.add(layers.Flatten())\r\n",
        "    model.add(layers.Dropout(0.5))\r\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\r\n",
        "    model.add(layers.Dense(NUM_LABELS, activation=\"sigmoid\"))\r\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(lr=1e-4), metrics=[\"acc\"])\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rgmW8mI0KPz"
      },
      "source": [
        "The input shape for the network is defined in the layer defintion of the networks first layer \r\n",
        "The returned model is ready for training.\r\n",
        "\r\n",
        "In order to realize the kfold validation the previously generated list of kfolds has to be iterated over. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhd7f-4-9Ogf"
      },
      "source": [
        "for i,fold in enumerate(folds):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNvMeJuX9PJe"
      },
      "source": [
        "In this loop for each fold the train and test dataset have to be loaded\r\n",
        "### 3.3.2 Loading the labels and images \r\n",
        "For the loading process based on the list of folds a process loading the labels and images from a list of filepaths had to be implemented.\r\n",
        "for this the following helper functions are used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smFZaVShAx1M"
      },
      "source": [
        "def get_file_list(dir):\r\n",
        "    #Liste mit kompletten Pfaden der Bilder\r\n",
        "    return [os.path.join(dir,bild) for bild in os.listdir(dir)]\r\n",
        "def get_labels(bilder):\r\n",
        "    matching_dict = ['_'+ os.path.basename(bild).split('_',1)[1].replace('.jpg','') for bild in bilder]\r\n",
        "    labels = np.array([dict_values.index(matching_str) for matching_str in matching_dict])\r\n",
        "    return to_categorical(labels,NUM_LABELS)\r\n",
        "def get_images(bilder):\r\n",
        "    num_images = len(bilder)\r\n",
        "    images = np.empty((num_images,IMG_H,IMG_W,IMG_C),dtype = np.float32)\r\n",
        "    for i,bild in enumerate(bilder):\r\n",
        "      image = load_img(bild, target_size = (IMG_H,IMG_W))\r\n",
        "      image = img_to_array(image)\r\n",
        "      image/=255\r\n",
        "      images[i,:,:,:] = image\r\n",
        "    return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMI3EhVcA-Yg"
      },
      "source": [
        ""
      ]
    }
  ]
}